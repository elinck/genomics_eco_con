[
  {
    "objectID": "genomics.html",
    "href": "genomics.html",
    "title": "genomics",
    "section": "",
    "text": "Quarto enables you to weave together content and executable code into a finished document. To learn more about Quarto see https://quarto.org."
  },
  {
    "objectID": "genomics.html#quarto",
    "href": "genomics.html#quarto",
    "title": "genomics",
    "section": "",
    "text": "Quarto enables you to weave together content and executable code into a finished document. To learn more about Quarto see https://quarto.org."
  },
  {
    "objectID": "hpc.html",
    "href": "hpc.html",
    "title": "Using MSU’s Tempest Cluster",
    "section": "",
    "text": "Up until now, each of you has been working on a different computer—your own. These differences include operating system (Mac vs. Windows vs. Linux), RAM, available storage capacity, number of CPUs, and preinstalled software. We’ve likely already run into problems because of this diversity: Different software versions use different commands, or have different security levels, for instance. As fun as troubleshooting has been, the remainder of the class will go much smoother if we only have to face installation and execution issues once—as a group. We will also be running software that requires RAM and computational resources far beyond what the typical student laptop can provide. For this reason, the goal of today’s lab is to become acquainted with the basics of MSU’s high-powered computing (HPC) cluster, Tempest.\nAs you will recall from your reading and discussion, Tempest is run by MSU’s Research Cyberinfrastructure group. The largest supercomputer in Montana, it provides researchers with an exceptionally powerful set of computational resources. Some of you may already have accounts on Tempest under your PI, who at minimum is able to access 256 CPUs, 1024GB of memory, and 2 GPUs (of which up to 1 can be an A100 GPU). These terms may still be unfamiliar, so let’s review them. Traditionally, a CPU (or central processing unit) was considered the basic unit of computing, capable of handling one operation at a time. However, this language is a holdover of an era where all CPUs were what is described as single-core, i.e., a single processor. Modern CPUs may have multiple cores: My laptop, for instance, has 8 cores within its CPU. Regardless, CPUs in a cluster are located on nodes, which you can think of as a standalone computer—i.e., something with its own storage and memory and one or CPUs—albeit one with more firepower than any personal machine. In an HPC, nodes are connected to each other over a network. Memory is what holds running programs and all their data. A GPU (which is beyond the scope of this class) is a special kind of super-fast processor that speeds things up by running jobs in parallel (i.e., at the same time).\nEach PI account on Tempest has its own partition. To understand this concept, it is helpful to consider a 3D abstraction of computation resources provided by RCI:\n\n\n\nTotal resources are defined by cores used x memory needed x runtime\n\n\nTempest is far more powerful than most of us will ever need. Yet it is still finite, and thus must balance simultaneous requests for resources among users. Partitions are part of its solution to this dilemma: on-demand memory, storage, and runtime for each lab. Within each lab, however, different accounts may compete for available CPUs and RAM, requiring a way to schedule tasks. (More on that below.) For now, it is sufficient to know that you will be accessing either your PI’s partition (if you’ve previously requested access) or the special partition set up for this class.\n\n\nOne of the many perks of Tempest is its web-based graphical user interface (GUI), which provides a familiar point-and-click environment to run programs like RStudio or Jupyter Notebooks. I am sorry to say that we will be completely ignoring Tempest Web in favor of its command-line-based version. This is not simply my rigid aesthetic preference. All or nearly all of the programs we will run are command-line only, and so navigating the HPC the old-fashioned way is a prerequisite to competence. We start by learning how to log onto the HPC via ssh (something known as a secure shell protocol). First, however, you’ll need to ensure you are connected to MSU’s VPN. Hopefully you had time to install this before class and give it a test run—clicking “Connect” will prompt login via NetID, followed by required two-factor authentication (2FA). Go ahead and connect to the VPN now. Assuming you are successful, the next step is to open up your terminal and enter the following command (replacing &lt;net_id&gt; with your own Net ID, of course):\nssh &lt;net_id&gt;@tempest-login.msu.montana.edu\nHaving done so, you should see your terminal interface change to reflect the following new file system:\n[&lt;net_id&gt;@tempest-login ~]$\nThis means you have successfully logged on to Tempest, and are now navigating a (virtual) computer on the HPC known as the “Login Node”. Type pwd. You should see a path reflecting either your PI’s account (/home/&lt;net_id&gt;) or an account linked to this class. Take a minute to navigate around and familiarize yourself with the files structure of the HPC and its contents using familiar commands (ls, cd). Note the directory bioe-591-genomics/ with its subdirectories students/ and course-materials/. In students/, make a new directory with your name (mkdir name). This can be a pre-submission home for the scripts and data used in this class moving forward.\n\n\n\n\n\n\nWarningCluster Safety\n\n\n\nDifferent HPCs allow different tiers of users different permissions, often as a way of preventing command-line accidents, e.g. typing the wrong bash one-liner and overwriting something important to the overall function of the machine. Regardless, you should assume that anything you type on the cluster has the potential to do unintended harm, and double-check it carefuly. In particular, apply rm combined with wildcards like * with extreme caution. Consider the consequences of accidentally deleting another user’s dissertation analysis or data. You’d likely skip the country at the very least; becoming a monastic and renouncing all earthly pleasures is a distinct possibility.\n\n\n\n\n\nNow that you are on Tempest, a natural question is how you might transfer the files and scripts you need for your own research onto the cluster. MSU’s answer—typical of research HPCs across the country—is Globus. Globus is what is known as a data transfer client, meaning an application allowing rapid movement of large files among secured locations in a network. You may be excited to learn that it has a GUI, and that unlike the GUI for Tempest, we will be taking advantage of it. You should also be broadly familiar with the purpose and use of Globus from your reading earlier this week. Ideally you’ve already installed Globus Connect Personal, too. If not, go ahead and do so now. Instructions for Mac and Windows are available here. Once downloaded and installed, you’ll need to open the app, grant it appropriate permissions, log in with institutional information, your Net ID, and password. The next step will be to establish a “Collection” on your laptop. This is simply Globus’ term for connecting to the broader Globus network in a way that will allow you to transfer data across different computers. Name your collection and exit set up after ensuring\nNext, return to your browser and navigate to the Globus Web interface. Click “LOG IN”, select “Montana State University - Bozeman”, and follow the prompts for your Net ID and 2FA. You should see a page similar to the following:\n These two panes show the file structures of two different nodes in the Globus network. In this class, this will always be Tempest and your own computer, but you may find yourself accessing MSU’s long-term storage solution (Blackmore) as well. To start, click on the left-hand pane and type “montana#tempest” (this may or may not autopopulate) and click “Search”. You should see a box with that name and three green tiles on the left-hand side pop up. Click this, and let it load. Assuming all is functioning, you will be rewarded with a visual view of the directories you previously navigated by the command line.\nNext, click on the right-hand pane. Search for the name of the collection on your laptop, or alternatively, click the “Your Collections” tab, where it may already be listed. In either case, double-click it once it appears, and you should see your own local file structure appear.\nYou are now ready to transfer your first file from your computer to Globus. For now, any plain text file will do; recall that if you would quickly create one with minimal contents, you may do so from the command line with (e.g.) the following commands:\ntouch globus_test.txt;\necho \"test\" &gt; globus_test.txt\nOn the left-hand side of Globus’ web interface, navigate to the students/ subdirectory of bioe-591-genomics, and from there to your own personal subdirectory. Click anywhere on the right-hand side of Globus, and navigate whatever directory contains the file you wish to transfer to Tempest. Select it, and click “Start”. A message should appear indicating you’ve begun the file transfer process; depending on the configuration of your devices, you may also recieve email notifications to this effect and confirming the eventual successful transfer. For a miniscule plain text file, this will be very fast—within a minute, the file should be visible in the Tempest portion of the browser. For larger files, this can take minutes to hours, but is nonetheless faster and more secure than common alternatives.\n\n\n\nA significant challenge in successfully running software from the command line is properly installing software and its dependencies (other software packages a given program relies upon to run). Manually loading each and every program required for a particular analysis is both tedious and error-prone, as mutual compatibility is often predicated on the particular version of a tool. If you’ve ever loaded a complex R package using CRAN and the install.packages() function, you’ve seen one solution to this problem. For many other types of software—particularly command-line tools and Python versions—an “environment manager” called Mamba is widely used.\nFirst, a little history. Mamba (documentation here) is the less-bloated, faster equivalent of another environment manager called Anaconda (or Conda; documentation here). Its commands are equivalent, so any software that provides command-line instructions for installing it via Conda can be loaded in the same way. This will make more sense once you see it in practice; for now, the important thing is to recognize Mamba as the primary way we will install bioinformatics and genomics tools on the cluster.\nOf course, Mamba is itself a program that must be installed. Luckily, Research Cyberinfrastructure has done so for us already. To access Mamba from the class Tempest partition (or your PIs), we will use a command-line tool called module. “Modules”—themselves a solution to the challenges of software installation—modify your path and configure your shell to allow you to run software that is installed at a higher level elsewhere on a computer or HPC. The details of this are outside our purview, but for now, recognize that running an application requires directing your computer to its location the way you would a standalone script (e.g., bash /user/you/homework/script.sh), and that your “environment” (the conditions of your user account) can be modified to avoid the need for absolute paths (e.g., they provide the ability to instead run bash script.sh from anywhere on the cluster when logged in).\nSeveral commands are helpful for working with modules: module avail will produce a LONG list of all modules loaded by RCI, while module search &lt;name_of_module&gt; can winnow this down to packages of interest. Feel free to try these; you may need to type q to exit their output once you’ve finished scrolling (with the arrow keys). Next, enter the following commands:\nmodule load Mamba/23.11.0-0;\nmamba\nThis will load the newest version of Mamba on your account, and print a set of instructions describing how the program is used to your screen. To further confirm the module you have loaded is the one you intended, you can type module list, which should spit back Mamba/23.11.0-0. Before we go further, it can be useful to know how to backtrack, i.e. clean your environment of modules to load something new (or a different version of the same thing). You can do so simply with module unload &lt;package&gt;, e.g. module unload Mamba/23.11.0-0 in the current case. (You may have noticed that typing the first few characters of a module name and pressing tab on your keyboard will authopopulate your screen with all possibilites that match that string up until that point. This can be useful for quickly showing versions——typing module load Mamba + tab after unloading Mamba/23.11.0-0 will print two alternatives. For now, stick with the most recent.)\nYou might think we are now ready to use Mamba itself. Unfortunately, there is one more cryptic step. Enter the following command into your terminal:\neval \"$(conda shell.bash hook)\"\nIn doing so, you will manipulate an environmental variable (something that only exists in the RAM of a particular shell / Terminal window / job) called $PATH. Your $PATH provides a list of directories that contain versions of software, allowing you to run them from the command line without their absolute paths. “But wait!”, I can hear you asking already. “Isn’t this what module load does?”. Yes, to an extent. However, unlike simpler programs, certain environment managers (the programs Conda and Mamba among them) need the ability to manipulate your shell itself (the *nix interface of the Terminal when accessing the HPC). The command above gives them this ability, albeit only once a session. (If this feels like magic, that’s fine. Once you’ve entered the command above, you can forget it until you need it again.)\nConfirm the command has taken effect by printing your $PATH to the screen:\necho $PATH\n(As long as paths with mamba or conda are visible in the toutput, it is likely working as intended.)\nThough it can facilitate multiple workflows, Mamba is arguably best used by creating named environments containing particular versions of software that can be loaded and unloaded as needed. To demonstrate its utility, we’ll work with a simple, efficient commandline tool called Seqtk. Seqtk is useful for converting sequence data between common formats, reporting summary statistics, merging files, and other basic operations. We will install Seqtk in its own environment, which we will create and give a unique name (“demo):\nmamba env create --name demo -c bioconda seqtk\nHere, create initializes a new environment, the --name argument sets the name of the evnironment, -c tells the program to look for the software on an archive called Bioconda, and seqtk is the name of the software. If you wished to install multiple software packages into the demo environment, you could add additional names after seqtk as needed. However, you would first need to confirm they are available via Conda / Mamba—something easily done by googling “Mamba ”, which brings up documentation like this, replete with command line “recipes” for installation.\nYou will be prompted to confirm the full set of dependencies and programs necessary to run seqtk. Type Y. The installation should finish, providing instructions for how to activate the environment. Ignore these for now. Type mamba env list. You should see demo and base appear, the latter referring to Mamba’s default state.\nConfusingly, we will activate this environment using a conda command, something that is an artifact of the Mamba version available via RCI module.\nconda activate demo\nYou should see (base) on the left-hand side of your terminal change to (demo). Verify you have loaded the proper environment by typing seqtk. If all has gone well, you should see a list of commands and their uses.\nAt this point, and at the risk of hopelessly confusing you, I am going to show you a second, preferable way to create environments. Let’s clean up by deactivating the current environment and then deleting it:\nconda deactivate;\nmamba env remove -n demo\nWe go about replacing it using a recipe encoded in something called a .yaml (or .yml) file, which are simply plain text files containing an easily interpreted language with information on software configurations and versions. This format pops up repeatedly (at the start of .Rmarkdown documents, for instance), so it is worth recognizing, even if it doesn’t deserve much thought. Create a file named demo_new.yml, and paste the following code into it using nano or another text editor:\nname: demo_new\nchannels:\n  - conda-forge\n  - bioconda\ndependencies:\n  - seqtk\nThis file provides detailed information to Mamba on exactly how to create a new environment. It is thus advantageous for any situation where you want your work to be reproduced with minimal hassle: running an analysis on a more powerful computer, sharing a pipeline with colleagues, or providing reviewers everything they need to generate the same results from your study. (The alternative—saving the long, one-off commands used to create environments in some sort of document—becomes cumbersome when multiple software packages are required, and is more prone to errors when used by others with different shell configurations.)\nWe can create (or recreate) a mamba environment using this file with the following command:\nmamba env create -f demo_new.yml\nAs before, activate the environment and confirm Seqtk is available to you:\nconda activate demo_new\nseqtk\n\n\n\nThe last thing we will cover as part of our introduction to using Tempest and other HPCs is job scheduling. Up until this point, everything we have done has occured on what is known as the log-in node—a computer with minimal memory, storage, and cores used to navigate directories, install software, and test simple commands that do not require substantial computational resources. Permissions and safety checks vary across machines, but in some contexts, attempting to do anything more on the log-in node can result in crashing the entire HPC, something I am sure you all want to avoid being responsible for. As discussed above, resource-intensive jobs must be requested using a job scheduler, which in our case is a program called SLURM. We will learn the basics of SLURM to run a simple seqtk command in our new Mamba environment.\nTo start, navigate to bioe-591-genomics/class-resources/scripts/. Typing ls will reveal a single file named demo.sbatch. Examine its contents (with cat or nano). You should see the following text appear:\n#!/bin/bash\n##\n## example-array.slurm.sh: submit an array of jobs with a varying parameter\n##\n## Lines starting with #SBATCH are read by Slurm. Lines starting with ## are comments.\n## All other lines are read by the shell.\n##\n#SBATCH --account=priority-bioe-591-genomics        #specify the account to use\n#SBATCH --job-name=demo                             # job name\n#SBATCH --partition=priority              # queue partition to run the job in\n#SBATCH --nodes=1                       # number of nodes to allocate\n#SBATCH --ntasks-per-node=1             # number of descrete tasks - keep at one except for MPI\n#SBATCH --cpus-per-task=1              # number of cores to allocate\n#SBATCH --time=0-00:10:00                 # Maximum job run time\n#SBATCH --output=seqtk_demo-%j.out\n#SBATCH --error=seqtk_demo-%j.err\n\n## Run 'man sbatch' for more information on the options above.\n\nmodule load Mamba/23.11.0-0\neval \"$(conda shell.bash hook)\"\nconda activate demo\necho \"reads     bases\" &gt; size.txt\nseqtk size ~/bioe-591-genomics/course-materials/data/intro/catamenia_analis.fastq.gz &gt;&gt; size.txt\nIn the text above, lines with two hashtags (##) are comments, while lines with a single hashtag (#) are instructions to Slurm. You should spend some time on Tempest’s instructions for how to use this job scheduler, but we will now briefly address its key components. The first executed line—#SBATCH --account=priority-bioe-591-genomics—indicates the account you are requesting resources on. If you have Tempest access for reasons other than this class (i.e., you are a member of a lab with its own partition), you may want to change this; you should already know the name of the partition, but in general these follow the form priority-firstnamelastname. The second line (#SBATCH --job-name=demo) is, obviously enough, the name of a particular job; you’ll want to change this as needed. The third line, #SBATCH --partition=priority, indicates that you are requesting the priority partition within your account. Doing so is almost always the right move for serious analyses. A detailed rundown of paritions is available here; note that the group and test partitions use resources from elsewhere on the cluster as available, and have runtime limits.\nOther critical settings include #SBATCH --nodes=1 , #SBATCH --cpus-per-task=1 #SBATCH --ntasks-per-node=1, and #SBATCH --time=0-00:10:00, which define the number of nodes, CPUs per task, nodes per task, and overall runtime you are requesting. Setting these can be an art; too low, and your analyses might drag on for all 14 days before timing out; too high, and it may never run as shorter jobs take priority in the queue. In general, I will make the resources you should request explicit, but for your independent work on the cluster in the future, know that these variables are key for troubleshooting.\nThe last lines in the prefix we will address are #SBATCH --output=seqtk_demo-%j.out and #SBATCH --error=seqtk_demo-%j.err. These define the names and locations of text files that record the standard output (*.out; what gets printed to the screen) and error messages (*.err) of the program(s) you are running. The character %j automatically populates the unique job number associated with your resource request, and should be left as is, but the name of both files should be changed accordingly.\nBelow the #SBATCH block are a set of commands that the HPC will actually execute using the resources requested. They should look mostly familiar, as the first three are what you’ve used to load the Mamba module, modify your $PATH to allow Mamba or Conda to load environments, and then load the demo environment you created earlier (in order to make the tool Seqtk available). The next line is a simple bash command to create a new file with two labeled columns separated by a tab. The final line calls seqtk to assess the quantity of DNA sequence data in the file catamenia_analis.fastq.gz. We will discuss this format in length next week, but for now, it may be helpful to know that it stores data from short-read sequencing of a set of exons from a single Band-tailed Seedeater.\nCopy this script to your own directory (cp ~/bioe-591-genomics/course-materials/scripts/demo.sbatch ~/bioe-591-genomics/students/user/.) Move it to a subdirectory for scripts if you like. Using nano, edit the job name, output, and error messages to something personal. Next, place your first Tempest resource request via the following command (run from the location of the copied file, not its original home in course-materials/):\nsbatch demo.sbatch\nYou should see a message reading Submitted batch job XXXXXXXX. Enter the following command:\nsacct\nThis will show all pending or running jobs on your account. Alternatively, you can use the following command to show only those jobs you as a user are running (as opposed to the several other classmates using the priority-bioe-591-genomics account):\nsqueue -u $USER\nThis particular job finishes almost instantly, so you’ll see COMPLETED as output from the first command, and empty data columns for the second. But for most tasks in bioinformatics and genomics, runtime is measured in minutes to hours to days, and using these commands can be a helpful check on whether the cluster is doing the work you expect it to be.\nSince the job is done, type ls. You’ll see that in the directory you called the script from, two files with the suffixes .out and .err have been generated. Recall that you specified the name of these in the ##SBATCH block of your Slurm script. Use cat or nano to inspect their contents. Both will be empty (unless something has gone very wrong), but in many cases, these files will be invaluable for troubleshooting why an analysis didn’t work the way you expected. Next, view site.txt. You should see two labeled columns (manually specified with echo in our script), with the relevant data underneath.\nWhat happens if you’ve made a mistake, or a job seems to have stalled out, and you’d like to cancel it? Using sacct to identify the job ID (also printed on the screen in response to running sbatch) allows you to selectively kill tasks:\nscancel JOBID\nLess precisely, you can cancel all tasks running on your account with the --user flag and your ID (visible in your shell as [myuserid@tempest-login ~]$).\nscancel --user=myuserid\nSeqtk is a very simple tool, but this basic process—configure an .sbatch script to request resources, use bash and / or mamba to load the relevant software and environment, run a command using a loaded tool, ocassionally killing tasks—is one that you will follow over and over again. You will be surprised (I hope!) at how quickly it becomes natural.\n\n\n\n\n\n\nNoteHomework 3\n\n\n\nToday’s homework assignment is intended to give you a little more practice using Slurm and tools like seqtk. As usual, I would like you to upload your script (in this case, an .sbatch file) to GitHub. The easiest way to do this is to use Globus to transfer it back to your laptop and local .git repository (though see the note below for an advanced alternative). Specifically, I’d like you to complete the following steps:\n\nCopy the demo .sbatch script used above. Edit any relevant variables in the #SBATCH configuration block.\nBelow the #SBATCH block, edit the commands as needed to randomly sample 1000 reads from the file catamenia_analis.fastq.gz and save the output as something unique in your local directory. A few hints follow.\n\n\nFirst, you’ll still need to load Mamba and the demo environment.\nSecond, the command beginning with echo that creates labeled columns for the output of seqtk size is no longer relevant.\nThird, you’ll need to look at the Seqtk documentation to figure out how to subsample reads. To do so you may either search the internet or load Mamba and activate the demo environment on the log-in node. This will not stick around for your .sbatch run—you’ll still need to include the commands to do so in your script—but it will allow you to type seqtk to see useage examples. Each listed command can then be paired with seqtk (e.g., seqtk size) to get additional details details.\n\n\nRun your script and confirm it works. Note that paths will be important: you do not have write permission in the course-materials directory, so saving the output to your own directory is critical.\n\n\n\n\n\n\n\n\n\nTipLinking GitHub to Tempest\n\n\n\nThough Globus is convenient and powerful, you may find yourself wanting to use Git and GitHub directly from the cluster. Doing so requires a different approach than configuring your own laptop due to how HPCs handle network security, involving something called an ssh key. To begin, navigate to your home directory (cd ~/.) on the log-in node and type the following command:\nssh-keygen -t ed25519 -C \"lamarck@mnhn.fr\"\nThis creates two hidden files: a private key (~/.ssh/id_ed25519) and a public key (~/.ssh/id_ed25519.pub) that function as passwords for authentication. Extract the password from your public key by typing cat ~/.ssh/id_ed25519.pub and then selecting the entire line that begins ssh-ed25519 AAAA.... Save this somewhere convenient, and navigate to GitHub. Under “Settings”, navigate to “SSH and GPG keys”, click “New SSH key”, name it something helpful, and paste the copied password in the “Key” field. Click save. You should now be able to work with Git as normal. To quickly test your setup, navigate to your repository for this class, click “Git Clone”, and switch to the “SSH” tab to securely clone it. Copy the command and paste it in your terminal on the log-in node of Tempest. It should look something like this:\ngit clone git@github.com:username/class_repo.git\nIf you then see the appropriate directory, you’re all set."
  },
  {
    "objectID": "hpc.html#logging-in",
    "href": "hpc.html#logging-in",
    "title": "Using MSU’s Tempest Cluster",
    "section": "",
    "text": "One of the many perks of Tempest is its web-based graphical user interface (GUI), which provides a familiar point-and-click environment to run programs like RStudio or Jupyter Notebooks. I am sorry to say that we will be completely ignoring Tempest Web in favor of its command-line-based version. This is not simply my rigid aesthetic preference. All or nearly all of the programs we will run are command-line only, and so navigating the HPC the old-fashioned way is a prerequisite to competence. We start by learning how to log onto the HPC via ssh (something known as a secure shell protocol). First, however, you’ll need to ensure you are connected to MSU’s VPN. Hopefully you had time to install this before class and give it a test run—clicking “Connect” will prompt login via NetID, followed by required two-factor authentication (2FA). Go ahead and connect to the VPN now. Assuming you are successful, the next step is to open up your terminal and enter the following command (replacing &lt;net_id&gt; with your own Net ID, of course):\nssh &lt;net_id&gt;@tempest-login.msu.montana.edu\nHaving done so, you should see your terminal interface change to reflect the following new file system:\n[&lt;net_id&gt;@tempest-login ~]$\nThis means you have successfully logged on to Tempest, and are now navigating a (virtual) computer on the HPC known as the “Login Node”. Type pwd. You should see a path reflecting either your PI’s account (/home/&lt;net_id&gt;) or an account linked to this class. Take a minute to navigate around and familiarize yourself with the files structure of the HPC and its contents using familiar commands (ls, cd). Note the directory bioe-591-genomics/ with its subdirectories students/ and course-materials/. In students/, make a new directory with your name (mkdir name). This can be a pre-submission home for the scripts and data used in this class moving forward.\n\n\n\n\n\n\nWarningCluster Safety\n\n\n\nDifferent HPCs allow different tiers of users different permissions, often as a way of preventing command-line accidents, e.g. typing the wrong bash one-liner and overwriting something important to the overall function of the machine. Regardless, you should assume that anything you type on the cluster has the potential to do unintended harm, and double-check it carefuly. In particular, apply rm combined with wildcards like * with extreme caution. Consider the consequences of accidentally deleting another user’s dissertation analysis or data. You’d likely skip the country at the very least; becoming a monastic and renouncing all earthly pleasures is a distinct possibility."
  },
  {
    "objectID": "hpc.html#transferring-files",
    "href": "hpc.html#transferring-files",
    "title": "Using MSU’s Tempest Cluster",
    "section": "",
    "text": "Now that you are on Tempest, a natural question is how you might transfer the files and scripts you need for your own research onto the cluster. MSU’s answer—typical of research HPCs across the country—is Globus. Globus is what is known as a data transfer client, meaning an application allowing rapid movement of large files among secured locations in a network. You may be excited to learn that it has a GUI, and that unlike the GUI for Tempest, we will be taking advantage of it. You should also be broadly familiar with the purpose and use of Globus from your reading earlier this week. Ideally you’ve already installed Globus Connect Personal, too. If not, go ahead and do so now. Instructions for Mac and Windows are available here. Once downloaded and installed, you’ll need to open the app, grant it appropriate permissions, log in with institutional information, your Net ID, and password. The next step will be to establish a “Collection” on your laptop. This is simply Globus’ term for connecting to the broader Globus network in a way that will allow you to transfer data across different computers. Name your collection and exit set up after ensuring\nNext, return to your browser and navigate to the Globus Web interface. Click “LOG IN”, select “Montana State University - Bozeman”, and follow the prompts for your Net ID and 2FA. You should see a page similar to the following:\n These two panes show the file structures of two different nodes in the Globus network. In this class, this will always be Tempest and your own computer, but you may find yourself accessing MSU’s long-term storage solution (Blackmore) as well. To start, click on the left-hand pane and type “montana#tempest” (this may or may not autopopulate) and click “Search”. You should see a box with that name and three green tiles on the left-hand side pop up. Click this, and let it load. Assuming all is functioning, you will be rewarded with a visual view of the directories you previously navigated by the command line.\nNext, click on the right-hand pane. Search for the name of the collection on your laptop, or alternatively, click the “Your Collections” tab, where it may already be listed. In either case, double-click it once it appears, and you should see your own local file structure appear.\nYou are now ready to transfer your first file from your computer to Globus. For now, any plain text file will do; recall that if you would quickly create one with minimal contents, you may do so from the command line with (e.g.) the following commands:\ntouch globus_test.txt;\necho \"test\" &gt; globus_test.txt\nOn the left-hand side of Globus’ web interface, navigate to the students/ subdirectory of bioe-591-genomics, and from there to your own personal subdirectory. Click anywhere on the right-hand side of Globus, and navigate whatever directory contains the file you wish to transfer to Tempest. Select it, and click “Start”. A message should appear indicating you’ve begun the file transfer process; depending on the configuration of your devices, you may also recieve email notifications to this effect and confirming the eventual successful transfer. For a miniscule plain text file, this will be very fast—within a minute, the file should be visible in the Tempest portion of the browser. For larger files, this can take minutes to hours, but is nonetheless faster and more secure than common alternatives."
  },
  {
    "objectID": "hpc.html#managing-environments-with-mamba",
    "href": "hpc.html#managing-environments-with-mamba",
    "title": "Using MSU’s Tempest Cluster",
    "section": "",
    "text": "A significant challenge in successfully running software from the command line is properly installing software and its dependencies (other software packages a given program relies upon to run). Manually loading each and every program required for a particular analysis is both tedious and error-prone, as mutual compatibility is often predicated on the particular version of a tool. If you’ve ever loaded a complex R package using CRAN and the install.packages() function, you’ve seen one solution to this problem. For many other types of software—particularly command-line tools and Python versions—an “environment manager” called Mamba is widely used.\nFirst, a little history. Mamba (documentation here) is the less-bloated, faster equivalent of another environment manager called Anaconda (or Conda; documentation here). Its commands are equivalent, so any software that provides command-line instructions for installing it via Conda can be loaded in the same way. This will make more sense once you see it in practice; for now, the important thing is to recognize Mamba as the primary way we will install bioinformatics and genomics tools on the cluster.\nOf course, Mamba is itself a program that must be installed. Luckily, Research Cyberinfrastructure has done so for us already. To access Mamba from the class Tempest partition (or your PIs), we will use a command-line tool called module. “Modules”—themselves a solution to the challenges of software installation—modify your path and configure your shell to allow you to run software that is installed at a higher level elsewhere on a computer or HPC. The details of this are outside our purview, but for now, recognize that running an application requires directing your computer to its location the way you would a standalone script (e.g., bash /user/you/homework/script.sh), and that your “environment” (the conditions of your user account) can be modified to avoid the need for absolute paths (e.g., they provide the ability to instead run bash script.sh from anywhere on the cluster when logged in).\nSeveral commands are helpful for working with modules: module avail will produce a LONG list of all modules loaded by RCI, while module search &lt;name_of_module&gt; can winnow this down to packages of interest. Feel free to try these; you may need to type q to exit their output once you’ve finished scrolling (with the arrow keys). Next, enter the following commands:\nmodule load Mamba/23.11.0-0;\nmamba\nThis will load the newest version of Mamba on your account, and print a set of instructions describing how the program is used to your screen. To further confirm the module you have loaded is the one you intended, you can type module list, which should spit back Mamba/23.11.0-0. Before we go further, it can be useful to know how to backtrack, i.e. clean your environment of modules to load something new (or a different version of the same thing). You can do so simply with module unload &lt;package&gt;, e.g. module unload Mamba/23.11.0-0 in the current case. (You may have noticed that typing the first few characters of a module name and pressing tab on your keyboard will authopopulate your screen with all possibilites that match that string up until that point. This can be useful for quickly showing versions——typing module load Mamba + tab after unloading Mamba/23.11.0-0 will print two alternatives. For now, stick with the most recent.)\nYou might think we are now ready to use Mamba itself. Unfortunately, there is one more cryptic step. Enter the following command into your terminal:\neval \"$(conda shell.bash hook)\"\nIn doing so, you will manipulate an environmental variable (something that only exists in the RAM of a particular shell / Terminal window / job) called $PATH. Your $PATH provides a list of directories that contain versions of software, allowing you to run them from the command line without their absolute paths. “But wait!”, I can hear you asking already. “Isn’t this what module load does?”. Yes, to an extent. However, unlike simpler programs, certain environment managers (the programs Conda and Mamba among them) need the ability to manipulate your shell itself (the *nix interface of the Terminal when accessing the HPC). The command above gives them this ability, albeit only once a session. (If this feels like magic, that’s fine. Once you’ve entered the command above, you can forget it until you need it again.)\nConfirm the command has taken effect by printing your $PATH to the screen:\necho $PATH\n(As long as paths with mamba or conda are visible in the toutput, it is likely working as intended.)\nThough it can facilitate multiple workflows, Mamba is arguably best used by creating named environments containing particular versions of software that can be loaded and unloaded as needed. To demonstrate its utility, we’ll work with a simple, efficient commandline tool called Seqtk. Seqtk is useful for converting sequence data between common formats, reporting summary statistics, merging files, and other basic operations. We will install Seqtk in its own environment, which we will create and give a unique name (“demo):\nmamba env create --name demo -c bioconda seqtk\nHere, create initializes a new environment, the --name argument sets the name of the evnironment, -c tells the program to look for the software on an archive called Bioconda, and seqtk is the name of the software. If you wished to install multiple software packages into the demo environment, you could add additional names after seqtk as needed. However, you would first need to confirm they are available via Conda / Mamba—something easily done by googling “Mamba ”, which brings up documentation like this, replete with command line “recipes” for installation.\nYou will be prompted to confirm the full set of dependencies and programs necessary to run seqtk. Type Y. The installation should finish, providing instructions for how to activate the environment. Ignore these for now. Type mamba env list. You should see demo and base appear, the latter referring to Mamba’s default state.\nConfusingly, we will activate this environment using a conda command, something that is an artifact of the Mamba version available via RCI module.\nconda activate demo\nYou should see (base) on the left-hand side of your terminal change to (demo). Verify you have loaded the proper environment by typing seqtk. If all has gone well, you should see a list of commands and their uses.\nAt this point, and at the risk of hopelessly confusing you, I am going to show you a second, preferable way to create environments. Let’s clean up by deactivating the current environment and then deleting it:\nconda deactivate;\nmamba env remove -n demo\nWe go about replacing it using a recipe encoded in something called a .yaml (or .yml) file, which are simply plain text files containing an easily interpreted language with information on software configurations and versions. This format pops up repeatedly (at the start of .Rmarkdown documents, for instance), so it is worth recognizing, even if it doesn’t deserve much thought. Create a file named demo_new.yml, and paste the following code into it using nano or another text editor:\nname: demo_new\nchannels:\n  - conda-forge\n  - bioconda\ndependencies:\n  - seqtk\nThis file provides detailed information to Mamba on exactly how to create a new environment. It is thus advantageous for any situation where you want your work to be reproduced with minimal hassle: running an analysis on a more powerful computer, sharing a pipeline with colleagues, or providing reviewers everything they need to generate the same results from your study. (The alternative—saving the long, one-off commands used to create environments in some sort of document—becomes cumbersome when multiple software packages are required, and is more prone to errors when used by others with different shell configurations.)\nWe can create (or recreate) a mamba environment using this file with the following command:\nmamba env create -f demo_new.yml\nAs before, activate the environment and confirm Seqtk is available to you:\nconda activate demo_new\nseqtk"
  },
  {
    "objectID": "hpc.html#scheduling-jobs-with-slurm",
    "href": "hpc.html#scheduling-jobs-with-slurm",
    "title": "Using MSU’s Tempest Cluster",
    "section": "",
    "text": "The last thing we will cover as part of our introduction to using Tempest and other HPCs is job scheduling. Up until this point, everything we have done has occured on what is known as the log-in node—a computer with minimal memory, storage, and cores used to navigate directories, install software, and test simple commands that do not require substantial computational resources. Permissions and safety checks vary across machines, but in some contexts, attempting to do anything more on the log-in node can result in crashing the entire HPC, something I am sure you all want to avoid being responsible for. As discussed above, resource-intensive jobs must be requested using a job scheduler, which in our case is a program called SLURM. We will learn the basics of SLURM to run a simple seqtk command in our new Mamba environment.\nTo start, navigate to bioe-591-genomics/class-resources/scripts/. Typing ls will reveal a single file named demo.sbatch. Examine its contents (with cat or nano). You should see the following text appear:\n#!/bin/bash\n##\n## example-array.slurm.sh: submit an array of jobs with a varying parameter\n##\n## Lines starting with #SBATCH are read by Slurm. Lines starting with ## are comments.\n## All other lines are read by the shell.\n##\n#SBATCH --account=priority-bioe-591-genomics        #specify the account to use\n#SBATCH --job-name=demo                             # job name\n#SBATCH --partition=priority              # queue partition to run the job in\n#SBATCH --nodes=1                       # number of nodes to allocate\n#SBATCH --ntasks-per-node=1             # number of descrete tasks - keep at one except for MPI\n#SBATCH --cpus-per-task=1              # number of cores to allocate\n#SBATCH --time=0-00:10:00                 # Maximum job run time\n#SBATCH --output=seqtk_demo-%j.out\n#SBATCH --error=seqtk_demo-%j.err\n\n## Run 'man sbatch' for more information on the options above.\n\nmodule load Mamba/23.11.0-0\neval \"$(conda shell.bash hook)\"\nconda activate demo\necho \"reads     bases\" &gt; size.txt\nseqtk size ~/bioe-591-genomics/course-materials/data/intro/catamenia_analis.fastq.gz &gt;&gt; size.txt\nIn the text above, lines with two hashtags (##) are comments, while lines with a single hashtag (#) are instructions to Slurm. You should spend some time on Tempest’s instructions for how to use this job scheduler, but we will now briefly address its key components. The first executed line—#SBATCH --account=priority-bioe-591-genomics—indicates the account you are requesting resources on. If you have Tempest access for reasons other than this class (i.e., you are a member of a lab with its own partition), you may want to change this; you should already know the name of the partition, but in general these follow the form priority-firstnamelastname. The second line (#SBATCH --job-name=demo) is, obviously enough, the name of a particular job; you’ll want to change this as needed. The third line, #SBATCH --partition=priority, indicates that you are requesting the priority partition within your account. Doing so is almost always the right move for serious analyses. A detailed rundown of paritions is available here; note that the group and test partitions use resources from elsewhere on the cluster as available, and have runtime limits.\nOther critical settings include #SBATCH --nodes=1 , #SBATCH --cpus-per-task=1 #SBATCH --ntasks-per-node=1, and #SBATCH --time=0-00:10:00, which define the number of nodes, CPUs per task, nodes per task, and overall runtime you are requesting. Setting these can be an art; too low, and your analyses might drag on for all 14 days before timing out; too high, and it may never run as shorter jobs take priority in the queue. In general, I will make the resources you should request explicit, but for your independent work on the cluster in the future, know that these variables are key for troubleshooting.\nThe last lines in the prefix we will address are #SBATCH --output=seqtk_demo-%j.out and #SBATCH --error=seqtk_demo-%j.err. These define the names and locations of text files that record the standard output (*.out; what gets printed to the screen) and error messages (*.err) of the program(s) you are running. The character %j automatically populates the unique job number associated with your resource request, and should be left as is, but the name of both files should be changed accordingly.\nBelow the #SBATCH block are a set of commands that the HPC will actually execute using the resources requested. They should look mostly familiar, as the first three are what you’ve used to load the Mamba module, modify your $PATH to allow Mamba or Conda to load environments, and then load the demo environment you created earlier (in order to make the tool Seqtk available). The next line is a simple bash command to create a new file with two labeled columns separated by a tab. The final line calls seqtk to assess the quantity of DNA sequence data in the file catamenia_analis.fastq.gz. We will discuss this format in length next week, but for now, it may be helpful to know that it stores data from short-read sequencing of a set of exons from a single Band-tailed Seedeater.\nCopy this script to your own directory (cp ~/bioe-591-genomics/course-materials/scripts/demo.sbatch ~/bioe-591-genomics/students/user/.) Move it to a subdirectory for scripts if you like. Using nano, edit the job name, output, and error messages to something personal. Next, place your first Tempest resource request via the following command (run from the location of the copied file, not its original home in course-materials/):\nsbatch demo.sbatch\nYou should see a message reading Submitted batch job XXXXXXXX. Enter the following command:\nsacct\nThis will show all pending or running jobs on your account. Alternatively, you can use the following command to show only those jobs you as a user are running (as opposed to the several other classmates using the priority-bioe-591-genomics account):\nsqueue -u $USER\nThis particular job finishes almost instantly, so you’ll see COMPLETED as output from the first command, and empty data columns for the second. But for most tasks in bioinformatics and genomics, runtime is measured in minutes to hours to days, and using these commands can be a helpful check on whether the cluster is doing the work you expect it to be.\nSince the job is done, type ls. You’ll see that in the directory you called the script from, two files with the suffixes .out and .err have been generated. Recall that you specified the name of these in the ##SBATCH block of your Slurm script. Use cat or nano to inspect their contents. Both will be empty (unless something has gone very wrong), but in many cases, these files will be invaluable for troubleshooting why an analysis didn’t work the way you expected. Next, view site.txt. You should see two labeled columns (manually specified with echo in our script), with the relevant data underneath.\nWhat happens if you’ve made a mistake, or a job seems to have stalled out, and you’d like to cancel it? Using sacct to identify the job ID (also printed on the screen in response to running sbatch) allows you to selectively kill tasks:\nscancel JOBID\nLess precisely, you can cancel all tasks running on your account with the --user flag and your ID (visible in your shell as [myuserid@tempest-login ~]$).\nscancel --user=myuserid\nSeqtk is a very simple tool, but this basic process—configure an .sbatch script to request resources, use bash and / or mamba to load the relevant software and environment, run a command using a loaded tool, ocassionally killing tasks—is one that you will follow over and over again. You will be surprised (I hope!) at how quickly it becomes natural.\n\n\n\n\n\n\nNoteHomework 3\n\n\n\nToday’s homework assignment is intended to give you a little more practice using Slurm and tools like seqtk. As usual, I would like you to upload your script (in this case, an .sbatch file) to GitHub. The easiest way to do this is to use Globus to transfer it back to your laptop and local .git repository (though see the note below for an advanced alternative). Specifically, I’d like you to complete the following steps:\n\nCopy the demo .sbatch script used above. Edit any relevant variables in the #SBATCH configuration block.\nBelow the #SBATCH block, edit the commands as needed to randomly sample 1000 reads from the file catamenia_analis.fastq.gz and save the output as something unique in your local directory. A few hints follow.\n\n\nFirst, you’ll still need to load Mamba and the demo environment.\nSecond, the command beginning with echo that creates labeled columns for the output of seqtk size is no longer relevant.\nThird, you’ll need to look at the Seqtk documentation to figure out how to subsample reads. To do so you may either search the internet or load Mamba and activate the demo environment on the log-in node. This will not stick around for your .sbatch run—you’ll still need to include the commands to do so in your script—but it will allow you to type seqtk to see useage examples. Each listed command can then be paired with seqtk (e.g., seqtk size) to get additional details details.\n\n\nRun your script and confirm it works. Note that paths will be important: you do not have write permission in the course-materials directory, so saving the output to your own directory is critical.\n\n\n\n\n\n\n\n\n\nTipLinking GitHub to Tempest\n\n\n\nThough Globus is convenient and powerful, you may find yourself wanting to use Git and GitHub directly from the cluster. Doing so requires a different approach than configuring your own laptop due to how HPCs handle network security, involving something called an ssh key. To begin, navigate to your home directory (cd ~/.) on the log-in node and type the following command:\nssh-keygen -t ed25519 -C \"lamarck@mnhn.fr\"\nThis creates two hidden files: a private key (~/.ssh/id_ed25519) and a public key (~/.ssh/id_ed25519.pub) that function as passwords for authentication. Extract the password from your public key by typing cat ~/.ssh/id_ed25519.pub and then selecting the entire line that begins ssh-ed25519 AAAA.... Save this somewhere convenient, and navigate to GitHub. Under “Settings”, navigate to “SSH and GPG keys”, click “New SSH key”, name it something helpful, and paste the copied password in the “Key” field. Click save. You should now be able to work with Git as normal. To quickly test your setup, navigate to your repository for this class, click “Git Clone”, and switch to the “SSH” tab to securely clone it. Copy the command and paste it in your terminal on the log-in node of Tempest. It should look something like this:\ngit clone git@github.com:username/class_repo.git\nIf you then see the appropriate directory, you’re all set."
  },
  {
    "objectID": "syllabus/01_syllabus.html",
    "href": "syllabus/01_syllabus.html",
    "title": "BIOE 591: Genomics for Ecology and Conservation",
    "section": "",
    "text": "Course: BIOE 591 (3 Credits)\nPrerequisite: BIOB480/BIOE548 or consent of instructor\nMeeting Time: T/TR 1:40 - 2:55 PM\nPlace: TBD\nInstructor: Dr. Ethan Linck (ethan.linck@montana.edu; 406-994-2024)\nOffice Hours: W: 1:00 – 4:00 PM\nMaterials: A laptop (tablets will not suffice)\n\n\nApplication of genomics methods to conservation, ecology, and evolution. Students gain familiarity with the basic tools and techniques of computational biology and genomics through case studies from the primary literature and analysis of empirical datasets. Examples and questions relevant to conservation biology and fish and wildlife management are emphasized.\n\n\n\nThe development of affordable high-throughput sequencing technology in the 2000s and 2010s revolutionized the ability of molecular ecologists, wildlife and conservation biologists, and other environmental scientists to assay genome-wide DNA sequence variation from nonmodel organisms. Genomics methods are now commonly used to study everything from the dietary preferences of sturgeon to genes underpinning local adaptation in wolves, but can be difficult to apply or interpret without specialist training. This course provides a hands-on introduction to the use of genomics in ecology, conservation, and related fields. Students will gain hands-on experience with computational biology, analyzing empirical genomic datasets with commonly applied software on a high-performance computing cluster. Reading assignments from the primary literature will demonstrate the use of focal software packages while emphasizing case studies relevant to conservation biology and fish and wildlife management.\n\n\n\nThe course will be offered as a mix of short lectures or demonstrations and discussions on reading assignments (typically Tuesdays) and supervised computer lab activities (typically Thursdays).\n\n\n\n\nCompare high-throughput sequencing approaches;\nEvaluate genomics methods sections in the primary literature;\nApply basic tools and techniques from computational biology;\nAnalyze empirical datasets with genomics softwate;\nCreate scripts for custom data analysis tasks.\n\n\n\n\nYour grade will be calculated as the fraction of points earned out of a total of 200. 120 points will come from completing lab assignments (10 points each; turned in the following class period as homework), 60 will come from posting comments or questions in Canvas discussion threads on assigned reading (5 points each; engagement with 12/14 papers needed), and 20 points will come from participation (based on attendance, clasroom participation, and assessment completion).\n\n\n\n\n\n\nWeek\nTopic\nAssignments\n\n\n\n\n1/12 - 1/16\nIntroduction and Review\n\nPreliminary Assessment\n\n\n\n1/19 - 1/23\nWhat is Genomics?\n\nHohenlohe et al. 2020\nHudson 2008*\nFelsenstein rant #1\n\n\n\n1/26 - 1/30\nComputational Biology Basics\n\nBraga et al. 2023\nSet up GitHub profiles with markdown READMEs (HW1)\n\n\n\n2/2 - 2/6\nComputing Clusters and the Command Line\n\nInstall sofware with mamba environment manager\nbash shell scripting activity (HW2)\n\n\n\n2/9 - 2/13\nSequencing strategies and\nShort Read Quality Control\n\nWatch Illumina video\nFuentes-Pardo et al. 2017\nFastQC Read filtering activity (HW3)\n\n\n\n2/16 - 2/20\neDNA and Metabarcoding\n\nLeempole et al. 2020\nBLAST activity (HW4)\n\n\n\n3/2 - 3/6\nPipelines and Genome Assembly\n\nEkblom & Wolf 2014\nSnakemake activity (HW5)\n\n\n\n3/9 - 3/13\nVariant Calling and Filtering\n\nShafer et al. 2017\nvcftools filtering activity (HW6)\n\n\n\n3/16 - 3/20\nSpring Break\n\n\n\n3/23 - 3/27\nKinship and Inbreeding\n\nHauser et al. 2022\nNgsRelate activity (HW7)\n\n\n\n3/30 - 4/3\nGenetic Diversity and Population Structure\n\nLinck & Battey 2019\nadegenet activity (HW8)\n\n\n\n4/6 - 4/10\nIntrogression and Hybridization\n\nMandeville et al. 2019\nentropy admixture activity (HW9)\n\n\n\n4/13 - 4/17\nPhylogenetics and Conservation Units\n\nLinck et al. 2019\nSVDquartets activity (HW10)\n\n\n\n4/20 - 4/24\nSimulations and Demographic History\n\nHoey et al. 2022\nCapblancq et al. 2020*\nfastsimcoal2 activity (HW11)\n\n\n\n4/27 - 5/1\nDetecting Natural Selection\n\nSchweizer et al. 2016\nbayenv activity (HW12)\n\n\n\n5/4 - 5/8\nFinals Week\n\n\n\n\n* denotes an optional reading assignment\n\n\n\n\nHohenlohe, P. A., Funk, W. C., & Rajora, O. P. (2021). Population genomics for wildlife conservation and management. Molecular Ecology, 30(1), 62-82.\nHudson, M. E. (2008). Sequencing breakthroughs for genomic ecology and evolutionary biology. Molecular Ecology Resources, 8(1), 3-17.\nhttps://felsenst.github.io/rants.html\nBraga, P. H. P., Hébert, K., Hudgins, E. J., Scott, E. R., Edwards, B. P., Sanchez Reyes, L. L., … & Crystal‐Ornelas, R. (2023). Not just for programmers: How GitHub can accelerate collaborative and reproducible research in ecology and evolution. Methods in Ecology and Evolution, 14(6), 1364-1380.\nFuentes‐Pardo, A. P., & Ruzzante, D. E. (2017). Whole‐genome sequencing approaches for conservation biology: Advantages, limitations and practical recommendations. Molecular Ecology, 26(20), 5369-5406.\nLeempoel, K., Hebert, T., & Hadly, E. A. (2020). A comparison of eDNA to camera trapping for assessment of terrestrial mammal diversity. Proceedings of the Royal Society B, 287(1918), 20192353.\nEkblom, R., & Wolf, J. B. (2014). A field guide to whole‐genome sequencing, assembly and annotation. Evolutionary Applications, 7(9), 1026-1042.\nShafer, A. B., Peart, C. R., Tusso, S., Maayan, I., Brelsford, A., Wheat, C. W., & Wolf, J. B. (2017). Bioinformatic processing of RAD‐seq data dramatically impacts downstream population genetic inference. Methods in Ecology and Evolution, 8(8), 907-917.\nHauser, S. S., Galla, S. J., Putnam, A. S., Steeves, T. E., & Latch, E. K. (2022). Comparing genome‐based estimates of relatedness for use in pedigree‐based conservation management. Molecular Ecology Resources, 22(7), 2546-2558.\nLinck, E., & Battey, C. J. (2019). Minor allele frequency thresholds strongly affect population structure inference with genomic data sets. Molecular Ecology Resources, 19(3), 639-647.\nLinck, E., Epperly, K., Van Els, P., Spellman, G. M., Bryson Jr, R. W., McCormack, J. E., … & Klicka, J. (2019). Dense geographic and genomic sampling reveals paraphyly and a cryptic lineage in a classic sibling species complex. Systematic Biology, 68(6), 956-966.\nHoey, J. A., Able, K. W., & Pinsky, M. L. (2022). Genetic decline and recovery of a demographically rebuilt fishery species. Molecular Ecology, 31(22), 5684-5698.\nCapblancq, T., Butnor, J. R., Deyoung, S., Thibault, E., Munson, H., Nelson, D. M., … & Keller, S. R. (2020). Whole‐exome sequencing reveals a long‐term decline in effective population size of red spruce (Picea rubens). Evolutionary Applications, 13(9), 2190-2205.\nSchweizer, R. M., Vonholdt, B. M., Harrigan, R., Knowles, J. C., Musiani, M., Coltman, D., … & Wayne, R. K. (2016). Genetic subdivision and candidate genes under selection in North American grey wolves. Molecular Ecology, 25(1), 380-402.\n\n\n\n\nMSU’s registration processes can be found on the Registrar’s website. January 27th is the last day to drop online; February 3rd is the last day to drop without a ‘W’ on your transcript; April 15th is the last day to drop with a ‘W’ on your transcript.\n\n\n\nGenerative AI tools are permitted to help debug code; they are not permitted as a shortcut to learning functions or commands from software documentation, and especially not permitted for summarizing reading assignments, where the point of the exercise is to become comfortable interpreting the technical genomics literature. I will not spend my time actively policing their use, but if detected, I will consider it academic misconduct.\n\n\n\nPlease do not come to campus if you are sick! I trust you will only miss class when absolutely necessary, and do not need extensive explanations for absences. However, please notify me as soon as practical, so that we can figure appropriate accommodations.\n\n\n\nI support an inclusive learning environment where diversity and individual differences are understood, respected, appreciated, and recognized as a source of strength. We expect that students, faculty, administrators and staff at MSU will respect differences and demonstrate diligence in understanding how other peoples’ perspectives, behaviors, and worldviews may be different from their own.\n\n\n\nIf you are a student with a disability and wish to use your approved accommodations for this course, please contact me during my office hours to discuss. Please have your Accommodation Notification or Blue Card available for verification of accommodations. Accommodations are approved through the Office of Disability Services located in SUB 174. Please see Disability Services for more information."
  },
  {
    "objectID": "syllabus/01_syllabus.html#catalog-description",
    "href": "syllabus/01_syllabus.html#catalog-description",
    "title": "BIOE 591: Genomics for Ecology and Conservation",
    "section": "",
    "text": "Application of genomics methods to conservation, ecology, and evolution. Students gain familiarity with the basic tools and techniques of computational biology and genomics through case studies from the primary literature and analysis of empirical datasets. Examples and questions relevant to conservation biology and fish and wildlife management are emphasized."
  },
  {
    "objectID": "syllabus/01_syllabus.html#overview",
    "href": "syllabus/01_syllabus.html#overview",
    "title": "BIOE 591: Genomics for Ecology and Conservation",
    "section": "",
    "text": "The development of affordable high-throughput sequencing technology in the 2000s and 2010s revolutionized the ability of molecular ecologists, wildlife and conservation biologists, and other environmental scientists to assay genome-wide DNA sequence variation from nonmodel organisms. Genomics methods are now commonly used to study everything from the dietary preferences of sturgeon to genes underpinning local adaptation in wolves, but can be difficult to apply or interpret without specialist training. This course provides a hands-on introduction to the use of genomics in ecology, conservation, and related fields. Students will gain hands-on experience with computational biology, analyzing empirical genomic datasets with commonly applied software on a high-performance computing cluster. Reading assignments from the primary literature will demonstrate the use of focal software packages while emphasizing case studies relevant to conservation biology and fish and wildlife management."
  },
  {
    "objectID": "syllabus/01_syllabus.html#course-organization-and-format",
    "href": "syllabus/01_syllabus.html#course-organization-and-format",
    "title": "BIOE 591: Genomics for Ecology and Conservation",
    "section": "",
    "text": "The course will be offered as a mix of short lectures or demonstrations and discussions on reading assignments (typically Tuesdays) and supervised computer lab activities (typically Thursdays)."
  },
  {
    "objectID": "syllabus/01_syllabus.html#learning-outcomes",
    "href": "syllabus/01_syllabus.html#learning-outcomes",
    "title": "BIOE 591: Genomics for Ecology and Conservation",
    "section": "",
    "text": "Compare high-throughput sequencing approaches;\nEvaluate genomics methods sections in the primary literature;\nApply basic tools and techniques from computational biology;\nAnalyze empirical datasets with genomics softwate;\nCreate scripts for custom data analysis tasks."
  },
  {
    "objectID": "syllabus/01_syllabus.html#grading",
    "href": "syllabus/01_syllabus.html#grading",
    "title": "BIOE 591: Genomics for Ecology and Conservation",
    "section": "",
    "text": "Your grade will be calculated as the fraction of points earned out of a total of 200. 120 points will come from completing lab assignments (10 points each; turned in the following class period as homework), 60 will come from posting comments or questions in Canvas discussion threads on assigned reading (5 points each; engagement with 12/14 papers needed), and 20 points will come from participation (based on attendance, clasroom participation, and assessment completion)."
  },
  {
    "objectID": "syllabus/01_syllabus.html#schedule",
    "href": "syllabus/01_syllabus.html#schedule",
    "title": "BIOE 591: Genomics for Ecology and Conservation",
    "section": "",
    "text": "Week\nTopic\nAssignments\n\n\n\n\n1/12 - 1/16\nIntroduction and Review\n\nPreliminary Assessment\n\n\n\n1/19 - 1/23\nWhat is Genomics?\n\nHohenlohe et al. 2020\nHudson 2008*\nFelsenstein rant #1\n\n\n\n1/26 - 1/30\nComputational Biology Basics\n\nBraga et al. 2023\nSet up GitHub profiles with markdown READMEs (HW1)\n\n\n\n2/2 - 2/6\nComputing Clusters and the Command Line\n\nInstall sofware with mamba environment manager\nbash shell scripting activity (HW2)\n\n\n\n2/9 - 2/13\nSequencing strategies and\nShort Read Quality Control\n\nWatch Illumina video\nFuentes-Pardo et al. 2017\nFastQC Read filtering activity (HW3)\n\n\n\n2/16 - 2/20\neDNA and Metabarcoding\n\nLeempole et al. 2020\nBLAST activity (HW4)\n\n\n\n3/2 - 3/6\nPipelines and Genome Assembly\n\nEkblom & Wolf 2014\nSnakemake activity (HW5)\n\n\n\n3/9 - 3/13\nVariant Calling and Filtering\n\nShafer et al. 2017\nvcftools filtering activity (HW6)\n\n\n\n3/16 - 3/20\nSpring Break\n\n\n\n3/23 - 3/27\nKinship and Inbreeding\n\nHauser et al. 2022\nNgsRelate activity (HW7)\n\n\n\n3/30 - 4/3\nGenetic Diversity and Population Structure\n\nLinck & Battey 2019\nadegenet activity (HW8)\n\n\n\n4/6 - 4/10\nIntrogression and Hybridization\n\nMandeville et al. 2019\nentropy admixture activity (HW9)\n\n\n\n4/13 - 4/17\nPhylogenetics and Conservation Units\n\nLinck et al. 2019\nSVDquartets activity (HW10)\n\n\n\n4/20 - 4/24\nSimulations and Demographic History\n\nHoey et al. 2022\nCapblancq et al. 2020*\nfastsimcoal2 activity (HW11)\n\n\n\n4/27 - 5/1\nDetecting Natural Selection\n\nSchweizer et al. 2016\nbayenv activity (HW12)\n\n\n\n5/4 - 5/8\nFinals Week\n\n\n\n\n* denotes an optional reading assignment"
  },
  {
    "objectID": "syllabus/01_syllabus.html#reading-list",
    "href": "syllabus/01_syllabus.html#reading-list",
    "title": "BIOE 591: Genomics for Ecology and Conservation",
    "section": "",
    "text": "Hohenlohe, P. A., Funk, W. C., & Rajora, O. P. (2021). Population genomics for wildlife conservation and management. Molecular Ecology, 30(1), 62-82.\nHudson, M. E. (2008). Sequencing breakthroughs for genomic ecology and evolutionary biology. Molecular Ecology Resources, 8(1), 3-17.\nhttps://felsenst.github.io/rants.html\nBraga, P. H. P., Hébert, K., Hudgins, E. J., Scott, E. R., Edwards, B. P., Sanchez Reyes, L. L., … & Crystal‐Ornelas, R. (2023). Not just for programmers: How GitHub can accelerate collaborative and reproducible research in ecology and evolution. Methods in Ecology and Evolution, 14(6), 1364-1380.\nFuentes‐Pardo, A. P., & Ruzzante, D. E. (2017). Whole‐genome sequencing approaches for conservation biology: Advantages, limitations and practical recommendations. Molecular Ecology, 26(20), 5369-5406.\nLeempoel, K., Hebert, T., & Hadly, E. A. (2020). A comparison of eDNA to camera trapping for assessment of terrestrial mammal diversity. Proceedings of the Royal Society B, 287(1918), 20192353.\nEkblom, R., & Wolf, J. B. (2014). A field guide to whole‐genome sequencing, assembly and annotation. Evolutionary Applications, 7(9), 1026-1042.\nShafer, A. B., Peart, C. R., Tusso, S., Maayan, I., Brelsford, A., Wheat, C. W., & Wolf, J. B. (2017). Bioinformatic processing of RAD‐seq data dramatically impacts downstream population genetic inference. Methods in Ecology and Evolution, 8(8), 907-917.\nHauser, S. S., Galla, S. J., Putnam, A. S., Steeves, T. E., & Latch, E. K. (2022). Comparing genome‐based estimates of relatedness for use in pedigree‐based conservation management. Molecular Ecology Resources, 22(7), 2546-2558.\nLinck, E., & Battey, C. J. (2019). Minor allele frequency thresholds strongly affect population structure inference with genomic data sets. Molecular Ecology Resources, 19(3), 639-647.\nLinck, E., Epperly, K., Van Els, P., Spellman, G. M., Bryson Jr, R. W., McCormack, J. E., … & Klicka, J. (2019). Dense geographic and genomic sampling reveals paraphyly and a cryptic lineage in a classic sibling species complex. Systematic Biology, 68(6), 956-966.\nHoey, J. A., Able, K. W., & Pinsky, M. L. (2022). Genetic decline and recovery of a demographically rebuilt fishery species. Molecular Ecology, 31(22), 5684-5698.\nCapblancq, T., Butnor, J. R., Deyoung, S., Thibault, E., Munson, H., Nelson, D. M., … & Keller, S. R. (2020). Whole‐exome sequencing reveals a long‐term decline in effective population size of red spruce (Picea rubens). Evolutionary Applications, 13(9), 2190-2205.\nSchweizer, R. M., Vonholdt, B. M., Harrigan, R., Knowles, J. C., Musiani, M., Coltman, D., … & Wayne, R. K. (2016). Genetic subdivision and candidate genes under selection in North American grey wolves. Molecular Ecology, 25(1), 380-402."
  },
  {
    "objectID": "syllabus/01_syllabus.html#drop-add-policy",
    "href": "syllabus/01_syllabus.html#drop-add-policy",
    "title": "BIOE 591: Genomics for Ecology and Conservation",
    "section": "",
    "text": "MSU’s registration processes can be found on the Registrar’s website. January 27th is the last day to drop online; February 3rd is the last day to drop without a ‘W’ on your transcript; April 15th is the last day to drop with a ‘W’ on your transcript."
  },
  {
    "objectID": "syllabus/01_syllabus.html#generative-ai",
    "href": "syllabus/01_syllabus.html#generative-ai",
    "title": "BIOE 591: Genomics for Ecology and Conservation",
    "section": "",
    "text": "Generative AI tools are permitted to help debug code; they are not permitted as a shortcut to learning functions or commands from software documentation, and especially not permitted for summarizing reading assignments, where the point of the exercise is to become comfortable interpreting the technical genomics literature. I will not spend my time actively policing their use, but if detected, I will consider it academic misconduct."
  },
  {
    "objectID": "syllabus/01_syllabus.html#attendance-policy",
    "href": "syllabus/01_syllabus.html#attendance-policy",
    "title": "BIOE 591: Genomics for Ecology and Conservation",
    "section": "",
    "text": "Please do not come to campus if you are sick! I trust you will only miss class when absolutely necessary, and do not need extensive explanations for absences. However, please notify me as soon as practical, so that we can figure appropriate accommodations."
  },
  {
    "objectID": "syllabus/01_syllabus.html#inclusivity-statement",
    "href": "syllabus/01_syllabus.html#inclusivity-statement",
    "title": "BIOE 591: Genomics for Ecology and Conservation",
    "section": "",
    "text": "I support an inclusive learning environment where diversity and individual differences are understood, respected, appreciated, and recognized as a source of strength. We expect that students, faculty, administrators and staff at MSU will respect differences and demonstrate diligence in understanding how other peoples’ perspectives, behaviors, and worldviews may be different from their own."
  },
  {
    "objectID": "syllabus/01_syllabus.html#syllabus-language-for-students-with-disabilities",
    "href": "syllabus/01_syllabus.html#syllabus-language-for-students-with-disabilities",
    "title": "BIOE 591: Genomics for Ecology and Conservation",
    "section": "",
    "text": "If you are a student with a disability and wish to use your approved accommodations for this course, please contact me during my office hours to discuss. Please have your Accommodation Notification or Blue Card available for verification of accommodations. Accommodations are approved through the Office of Disability Services located in SUB 174. Please see Disability Services for more information."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "BIOE591: Genomics for Ecology & Conservation",
    "section": "",
    "text": "Course: BIOE 591 (3 Credits)\nPrerequisite: BIOB480/BIOE548 or consent of instructor\nMeeting Time: T/TR 1:40 PM – 2:55 PM\nPlace: Wilson 1-154\nInstructor: Dr. Ethan Linck (ethan.linck@montana.edu; 406-994-2024)\nOffice Hours: 1:00 PM – 4:00 Weds\nMaterials: A laptop (tablets will not suffice). Windows users will need to download and install Git for Windows as a Unix shell emulator."
  },
  {
    "objectID": "index.html#catalog-description",
    "href": "index.html#catalog-description",
    "title": "BIOE591: Genomics for Ecology & Conservation",
    "section": "Catalog Description",
    "text": "Catalog Description\nApplication of genomics methods to conservation, ecology, and evolution. Students gain familiarity with the basic tools and techniques of computational biology and genomics through case studies from the primary literature and analysis of empirical datasets. Examples and questions relevant to conservation biology and fish and wildlife management are emphasized."
  },
  {
    "objectID": "index.html#overview",
    "href": "index.html#overview",
    "title": "BIOE591: Genomics for Ecology & Conservation",
    "section": "Overview",
    "text": "Overview\nThe development of affordable high-throughput sequencing technology in the 2000s and 2010s revolutionized the ability of molecular ecologists, wildlife and conservation biologists, and other environmental scientists to assay genome-wide DNA sequence variation from nonmodel organisms. Genomics methods are now commonly used to study everything from the dietary preferences of sturgeon to genes underpinning local adaptation in wolves, but can be difficult to apply or interpret without specialist training. This course provides a hands-on introduction to the use of genomics in ecology, conservation, and related fields. Students will gain hands-on experience with computational biology, analyzing empirical genomic datasets with commonly applied software on a high-performance computing cluster. Reading assignments from the primary literature will demonstrate the use of focal software packages while emphasizing case studies relevant to conservation biology and fish and wildlife management."
  },
  {
    "objectID": "index.html#course-organization-and-format",
    "href": "index.html#course-organization-and-format",
    "title": "BIOE591: Genomics for Ecology & Conservation",
    "section": "Course Organization and Format",
    "text": "Course Organization and Format\nThe course will be offered as a mix of short lectures or demonstrations and discussions on reading assignments (typically Tuesdays) and supervised computer lab activities (typically Thursdays)."
  },
  {
    "objectID": "index.html#learning-outcomes",
    "href": "index.html#learning-outcomes",
    "title": "BIOE591: Genomics for Ecology & Conservation",
    "section": "Learning Outcomes",
    "text": "Learning Outcomes\n\nCompare high-throughput sequencing approaches;\nEvaluate genomics methods sections in the primary literature;\nApply basic tools and techniques from computational biology;\nAnalyze empirical datasets with genomics softwate;\nCreate scripts for custom data analysis tasks."
  },
  {
    "objectID": "index.html#grading",
    "href": "index.html#grading",
    "title": "BIOE591: Genomics for Ecology & Conservation",
    "section": "Grading",
    "text": "Grading\nYour grade will be calculated as the fraction of points earned out of a total of 200. 120 points will come from completing lab assignments (10 points each; turned in the following class period as homework), 60 will come from posting comments or questions in Canvas discussion threads on assigned reading (5 points each; engagement with 12/14 papers needed), and 20 points will come from participation (based on attendance, leading at least one paper discussion and participating in the classroom, and completing the assessment)."
  },
  {
    "objectID": "index.html#resources",
    "href": "index.html#resources",
    "title": "BIOE591: Genomics for Ecology & Conservation",
    "section": "Resources",
    "text": "Resources\nI assume a solid grasp of basic genetics and evolution. If you consider yourself rusty in these areas, I recommend reviewing the lecture notes from my Conservation Genetics course (BIOB480 / BIOE548). I take for granted that you will have computer skills typical of a first-year graduate student in the middle of the third decade of the 21st century. This means: proficiency with what Gotelli calls the “unholy trinity” of M.S. Word, Excel and Powerpoint; a knowledge of basic computing hardware and operating systems; a knowledge of where to find things on your computer; and some background with scientific programming, even if shallow.\nThroughout the semester, you will likely find yourself resorting to Google, ChatGPT, and other resources to help run software and troubleshoot error messages. The following cheat sheets, tutorials and courses may also be of interest:\n\nOfficial GitHub Cheatsheet\n\nGit Beginner Cheatsheet\nCommand Line Cheatsheet #1\nCommand Line Cheatsheet #2\nSoftware Carpentry: The Unix Shell\nSoftware Carpentry: Programming with Python\nSoftware Carpentry: R for Reproducible Scientfic Analysis\nSoftware Carpentry: Snakemake for Bioinformatics\nThe Caprentries Incubator: Getting Started with Mamba\n\nHPC Carpentry\n\nThe Biologists’ Guide to Computing"
  },
  {
    "objectID": "index.html#schedule",
    "href": "index.html#schedule",
    "title": "BIOE591: Genomics for Ecology & Conservation",
    "section": "Schedule",
    "text": "Schedule\n\n\n\nWeek\nTopic\nAssignments\n\n\n\n\n1/12 - 1/16\nWhat is Genomics?\n\nPreliminary Assessment\nHohenlohe et al. 2020 (pdf)\nFelsenstein rant #1\n\n\n\n1/19 - 1/23\nMarkdown & the Command Line\n\nMarkdown & Command Line Basics (HW1)\nSandve et al. 2013 (pdf)\nBrandies & Hogg 2021 (pdf)\n\n\n\n1/26 - 1/30\nVersion Control with Git & Github\n\nGit & GitHub Basics (HW2)\nBlischak et al. 2016 (pdf)\nBraga et al. 2023 (pdf)\n\n\n\n2/2 - 2/6\nComputing Clusters\n\nIntroduction to Tempest (HW3)\nAlnasir 2021 (pdf)\nTempest Documentation\nGlobus Documentation\nMSU VPN Documentation\n\n\n\n2/9 - 2/13\nSequencing Strategies and Short Read Quality Control\n\nFiltering .fastq Files (HW4)\nIllumina Sequencing-by-Synthesis Video\nFuentes-Pardo & Ruzzante 2017 (pdf)\nToews et al. 2016 (optional) (pdf)\n\n\n\n2/16 - 2/20\neDNA and Metabarcoding\n\n\n\n3/2 - 3/6\nPipelines and Genome Assembly\n\n\n\n3/9 - 3/13\nVariant Calling and Filtering\n\n\n\n3/16 - 3/20\nSpring Break\n\n\n\n3/23 - 3/27\nKinship and Inbreeding\n\n\n\n3/30 - 4/3\nGenetic Diversity and Population Structure\n\n\n\n4/6 - 4/10\nIntrogression and Hybridization\n\n\n\n4/13 - 4/17\nPhylogenetics and Conservation Units\n\n\n\n4/20 - 4/24\nSimulations and Demographic History\n\n\n\n4/27 - 5/1\nDetecting Natural Selection\n\n\n\n5/4 - 5/8\nFinals Week"
  },
  {
    "objectID": "index.html#drop-add-policy",
    "href": "index.html#drop-add-policy",
    "title": "BIOE591: Genomics for Ecology & Conservation",
    "section": "Drop / Add Policy",
    "text": "Drop / Add Policy\nMSU’s registration processes can be found on the Registrar’s website. January 27th is the last day to drop online; February 3rd is the last day to drop without a ‘W’ on your transcript; April 15th is the last day to drop with a ‘W’ on your transcript."
  },
  {
    "objectID": "index.html#generative-ai",
    "href": "index.html#generative-ai",
    "title": "BIOE591: Genomics for Ecology & Conservation",
    "section": "Generative AI",
    "text": "Generative AI\nGenerative AI tools are permitted to help debug code; they are not permitted as a shortcut to learning functions or commands from software documentation, and especially not permitted for summarizing reading assignments, where the point of the exercise is to become comfortable interpreting the technical genomics literature. I will not spend my time actively policing their use, but if detected, I will consider it academic misconduct."
  },
  {
    "objectID": "index.html#attendance-policy",
    "href": "index.html#attendance-policy",
    "title": "BIOE591: Genomics for Ecology & Conservation",
    "section": "Attendance Policy",
    "text": "Attendance Policy\nPlease do not come to campus if you are sick! I trust you will only miss class when absolutely necessary, and do not need extensive explanations for absences. However, please notify me as soon as practical, so that we can figure appropriate accommodations."
  },
  {
    "objectID": "index.html#inclusivity-statement",
    "href": "index.html#inclusivity-statement",
    "title": "BIOE591: Genomics for Ecology & Conservation",
    "section": "Inclusivity Statement",
    "text": "Inclusivity Statement\nI support an inclusive learning environment where diversity and individual differences are understood, respected, appreciated, and recognized as a source of strength. We expect that students, faculty, administrators and staff at MSU will respect differences and demonstrate diligence in understanding how other peoples’ perspectives, behaviors, and worldviews may be different from their own."
  },
  {
    "objectID": "index.html#syllabus-language-for-students-with-disabilities",
    "href": "index.html#syllabus-language-for-students-with-disabilities",
    "title": "BIOE591: Genomics for Ecology & Conservation",
    "section": "Syllabus Language for Students with Disabilities",
    "text": "Syllabus Language for Students with Disabilities\nIf you are a student with a disability and wish to use your approved accommodations for this course, please contact me during my office hours to discuss. Please have your Accommodation Notification or Blue Card available for verification of accommodations. Accommodations are approved through the Office of Disability Services located in SUB 174. Please see Disability Services for more information."
  },
  {
    "objectID": "cli.html",
    "href": "cli.html",
    "title": "Markdown & Command Line Basics",
    "section": "",
    "text": "For much of the semester we will be executing programs from a command-line interface, or CLI. In scientific computing, a CLI known as the Unix or Linux Shell (sometimes abbreviated *NIX Shell; see if you can figure out why) is most commonly used and a requirement for many scripts and applications. On Mac and Linux operating systems, the CLI is accessed via an application called Terminal, which by default runs a version of the Unix shell called zsh or bash.\nWindows users–though likely already equipped with a CLI called PowerShell or CMD—will need to install a Unix shell emulator, as commands and syntax differs with these interpreters. Unless you have prior knowledge and a strong preference otherwise, I recommend Git for Windows, which comes with a BASH emulator that will work for all lab assignments.\n\n\nTo write code snippets and submit assignments, you will need to have a plain text editor to work with. Plain text files have numerous advantages over writing in (e.g.) a word processing application. First, they are free and open source, something that may become important to you once you lose an academic software license. Second, the files will always remain usable and readable by humans, whereas if Microsoft Word sunsets its proprietary software, .docx files may become useless. Third, they are more or less universally interpretable by other programs (i.e., they are highly portable); this allows you to input data and function arguments to software in automated fashion.\nMac, Linux, and Windows computers all typically come with a preloaded application for this purpose (e.g., textEdit on MacOS). You will likely enjoy a slightly more sophisticated editor aimed at writing code, which typically come with features like syntax highlighting and tab autofill / spellcheck (as well as a suite of AI features and easy version control integration that I will discourage you from using until you know the basics). RStudio provides these features, though as a full-on integrated development environment (or IDE) it is somewhat distracting. In rough order of declining preference, here are my suggestions:\n\nBBEdit (classic; paid and free tiers, though free is more than sufficient)\nSublimeText (works pretty well without a subscription, though you’ll get nagged)\nVSCode (powerful, free, annoying AI integration)\nVim or NeoVim (the hardcore dork option; if you show me you are effectively using this for your assignments, you get one get-out-of-jail free card for homework)\nNotepad++ (the best Windows editor!)\n\nWriting in plain text necessarily means forgoing formatting. Word processing is handled by a lightweight plain-text-to-formatted-text language called Markdown. Markdown is portable, simple, and ubiquitous: it is responsible for formatting this assignment, this course website, my lab website, my lecture slides, GitHub READMEs, Reddit posts, and much more besides. The basic idea is that by surrounding words with a handful of characters, you indicate to an interpreter how text should be formatted. For example, in this sentence I have surrounded the last six words with a pair of two asterices to indicate it should be bold:\n**to indicate it should be bold:**  \nItalics work with a single asterisk at the end of each word, like this:\n*this*:\nHeaders can be rendered with hashtags (# Title, ## Section, ### Subsection). Code can indicated by a pair of accents (`bash.sh`); three in a row on one opens a block of formatted code, which must then be closed by three further down. Depending on the flavor of Markdown, you can indicate syntax highlighting by putting the name of the programming language in brackets after the opening line of accents:\n\nprint(\"BIOE591\")\n\n[1] \"BIOE591\"\n\n\nTables are wonderfully simple. For example, the following text…\n| Student     | Fun Fact             |\n| ----------- | -------------------- |\n| Jason       | Doesn't like to walk |\n| Lizzy       | Has a chubby cat     |\n…renders as:\n\n\n\nStudent\nFun Fact\n\n\n\n\nJason\nDoesn’t like to walk\n\n\nLizzy\nHas a chubby cat\n\n\n\nBlock quotes can be indicated by greater-than signs (&gt;), e.g.\n&gt; Nonethless, Jason is still better at walking than Lizzy's cat.\nbecomes:\n\nNonethless, Jason is still better at walking than Lizzy’s cat.\n\nNumbered lists are equally simple, with the following chunk…\n1. Syllabus\n   -  Stern words\n2. Tease Jason\n   - Relent\n…rendering as:\n\nSyllabus\n\nStern words\n\n\nTease Jason\n\nRelent\n\n\n(Bullet points are handled as you might imagine.)\nA cheat sheet to basic and extended syntax can be found here. The web application JotBird is nice for quickly drafting Markdown documents; you may also be interested in downloading a program that can render Markdown as .pdf or .html files locally, such as Pandoc (recommended). (RMarkdown has this ability as well, though you’ll have to download LaTeX via TinyTeX, MacTeX, or another source.)\n\n\n\nAfter opening Terminal, it’s time to get oriented and learn how to navigate a computer via the CLI. To start, we will figure out where in your file structure you actually are, using the equivalent of the R function getwd():\npwd\nEasily memorized as print working directory, this should indicate you are in your home directory (e.g., /Users/ethanlinck/), a location you can return from wherever you are by typing cd ~ and hitting enter. cd is a fundamental tool of any CLI, allowing you to change directories:\ncd / # switches to the root directory, i.e. the highest level in the hierarchical structure of file storage\ncd .. # move up one level in the file structure hierarchy, i.e. from /Users/ethanlinck/ to /Users/\ncd - # switch to the previous directory\nIt can also use explicit paths. Here is an example with a relative path (from my current home directory, /Users/ethanlinck/):\ncd teaching/genomics/\nThis would navigate to the genomics subdirectory of my teaching directory from my current location. If I am accidentally not in my home directory and there are no such folders where I am, the command will fail. An absolute path helps avoid this risk, though at the cost of flexibility:\ncd /Users/ethanlinck/teaching/genomics/\nUse cd and pwd in combination to navigate around your computer. Note that if you are correctly typing the start of a path, pressing “tab” should autocomplete it, or present options with the same suffix..\nYou can make a directory with the command mkdir and a path (including the name of the new directory):\nmkdir /teaching/genomics/\nMake a new directory entitled test/. We can then create a set of new files to put in test/ for future manipulation using echo, the assignment pipe &gt;, and a filename:\necho \"I love learning\" &gt; ~/test/text1.md; \necho \"I love computers\" &gt; ~/test/text2.md; \necho \"I regret my choices\" &gt; ~/test/.secret;\nmkdir ~/test/sub/ \n(Each line below can be entered individually; the semicolons allow you to copy and past the chunk below without manually entering linebreaks. If you misplace a semicolon or otherwise have a typo, type Ctrl+C to cancel a command. Typing exit closes the current terminal session.)\nNavigate to ~/test/. From a given location—or paired with a path (written in help documents as &lt;path&gt;) you can use the command ls to list the contents of a directory:\nls  ~/test/\nThe command ls -a will reveal “dotfiles”, typically hidden text files that begin with a period and contain information that helps software run. In this class, the dotfile .gitignore will be useful down the line; we may also manipulate .bashrc, which determines settings for the unix shell of a particular user. What are the differences between ls and ls -a when run in test/?\nAt this point you may be wondering where the argument -a came from. The following commands produce documentation (though typing an erroneous argument will also provide a brief example of proper useage)\nman lc #zsh\nlc -h #bash\nCommands can be chained together using a pipe (something you may be familiar with from R). Here I am counting the number of files in a directory using the commands ls and wc, with appropriate arguments:\nls -1 ~/test/ | wc -l # list files in a directory, 1 per line; send output to wc function, count lines\nThe command cat can be used to print (or concatenate) the contents of a file:\ncat ~/test/text1.md\nSimilarly, head can be used to print the first -n lines of a file. We will demonstrate this with a new file and the application of a double pipe (which appends text to new lines):\necho \"test\" &gt; file.md\necho \"test\" &gt;&gt; file.md\necho \"test\" &gt;&gt; file.md\necho \"test\" &gt;&gt; file.md\necho \"test\" &gt;&gt; file.md\necho \"test\" &gt;&gt; file.md\nhead -5 file.md \nThe command mv can be used to move a file to a new location, e.g:\nmv ~/test/text1.md ~/test/sub/text1.md\nThis can also be used to rename files, even within the same directory:\nmv ~/test/sub/text1.md ~/test/sub/text0.md\nAn analog is cp, which copies files:\ncp ~/test/sub/text0.md ~/test/text0.md\nUsing a . in combination with a relative or absolute path will preserve the name of the original file:\ncp ~/test/file.md ~/test/sub/. \nPasting multiple commands in a text document with the suffix .sh creates a shell script, analogous to one written in R or Python. To do so, you need a hashed line called a shebang to start your script to tell your computer to use a particular CLI. You can then add multiple commands, separated by semi-colons:\n#!/usr/bin/env bash\ncd ~/test/; \necho \"lastly\" &gt; newfile.md;\nmv newfile.md ~/test/.;\necho \"file moved\"\nSave this as script.sh in the test/ directory. To do so you may use your new plain text editor, or type nano to access a CLI-based solution available on Mac and Linux machines. (This takes a second to get used to; ask me if you need help. ) Type the command bash script.sh. Does it do what you expect?\nWildcards are characters that match multiple patterns. Most useful for our purposes is *, which will match all files in a given directory, or all\nls * # matches all files in the present directory\nls *.md # matches all files with the suffix \".md\"\nAnother useful wildcard is ?, which matches a single character:\necho \"test\" &gt; tile.md\nls ~/test/?ile.md \nLastly, the command rm removes (deletes) files and directories. For example, rm test/sub/file.md deletes the file test/sub/file.md. Paired with wildcards, you can quickly remove many files in one fell swoop.\nrm ~/test/* # removes all files, but not subdirectories\nrm -r ~/test/ # deletes the directory itself using the \"recursive\" argument -r\n\n\n\n\n\n\nWarningUsing rm with caution\n\n\n\nOne pitfall of becoming a CLI ninja is that you will not get warning messages when you inevitably deploy a powerful command somewhere you don’t intend to. rm paired with a wildcard should be used especially sparingly. What do you think would happen if you navigated to your Desktop and typed the following command?\nrm *\n\n\n\n\n\n\n\n\nNoteHomework 1\n\n\n\nWe will ease into our coding exercises with a brief shell scripting activity. I have created a set of dummy data files that are typical of population genomics—.fastq.gz raw sequencing read files, .fasta assembled sequence data, text files (*.txt), and metadata (*.csv) files. First, download these data using the command curl:\ncurl -L -O https://raw.githubusercontent.com/elinck/genomics_eco_con/main/data/week_1.tar.gz -o week_1.tar.gz # download a resource at a url\ntar -xzf week_1.tar.gz # this command unzips a \"tarball\"\ncd week_1 # enter the data directory\nUsing the information above and any other resource available to you*, write a single bash script that can perform the following steps:\nWrite a bash script called 01_homework_&lt;your_name&gt;.sh that does the following:\n\nChange into the week_1 directory;\nCreate three subdirectories called:\n\n\nfastq/\nfasta/\nmetadata/\n\n\nMove files into these directories based on file extension:\n\n\n*.fastq.gz to fastq/\n*.fasta to fasta/\n*.csv to metadata/\n\n\nCount how many files are in each new directory and print to the screen.\n\nFor now, you may keep this in your local directory; we’ll work on submitting it as part of next week’s activities.\n* This will generally include AI tools, and as these are an indispensible part of modern programming, I am hesitant to outright ban them for this course. However, it will be in your best interest to attempt to put this script together yourself—from its component parts—as these commands need to become second nature as you navigate the cluster."
  },
  {
    "objectID": "cli.html#plain-text-markdown",
    "href": "cli.html#plain-text-markdown",
    "title": "Markdown & Command Line Basics",
    "section": "",
    "text": "To write code snippets and submit assignments, you will need to have a plain text editor to work with. Plain text files have numerous advantages over writing in (e.g.) a word processing application. First, they are free and open source, something that may become important to you once you lose an academic software license. Second, the files will always remain usable and readable by humans, whereas if Microsoft Word sunsets its proprietary software, .docx files may become useless. Third, they are more or less universally interpretable by other programs (i.e., they are highly portable); this allows you to input data and function arguments to software in automated fashion.\nMac, Linux, and Windows computers all typically come with a preloaded application for this purpose (e.g., textEdit on MacOS). You will likely enjoy a slightly more sophisticated editor aimed at writing code, which typically come with features like syntax highlighting and tab autofill / spellcheck (as well as a suite of AI features and easy version control integration that I will discourage you from using until you know the basics). RStudio provides these features, though as a full-on integrated development environment (or IDE) it is somewhat distracting. In rough order of declining preference, here are my suggestions:\n\nBBEdit (classic; paid and free tiers, though free is more than sufficient)\nSublimeText (works pretty well without a subscription, though you’ll get nagged)\nVSCode (powerful, free, annoying AI integration)\nVim or NeoVim (the hardcore dork option; if you show me you are effectively using this for your assignments, you get one get-out-of-jail free card for homework)\nNotepad++ (the best Windows editor!)\n\nWriting in plain text necessarily means forgoing formatting. Word processing is handled by a lightweight plain-text-to-formatted-text language called Markdown. Markdown is portable, simple, and ubiquitous: it is responsible for formatting this assignment, this course website, my lab website, my lecture slides, GitHub READMEs, Reddit posts, and much more besides. The basic idea is that by surrounding words with a handful of characters, you indicate to an interpreter how text should be formatted. For example, in this sentence I have surrounded the last six words with a pair of two asterices to indicate it should be bold:\n**to indicate it should be bold:**  \nItalics work with a single asterisk at the end of each word, like this:\n*this*:\nHeaders can be rendered with hashtags (# Title, ## Section, ### Subsection). Code can indicated by a pair of accents (`bash.sh`); three in a row on one opens a block of formatted code, which must then be closed by three further down. Depending on the flavor of Markdown, you can indicate syntax highlighting by putting the name of the programming language in brackets after the opening line of accents:\n\nprint(\"BIOE591\")\n\n[1] \"BIOE591\"\n\n\nTables are wonderfully simple. For example, the following text…\n| Student     | Fun Fact             |\n| ----------- | -------------------- |\n| Jason       | Doesn't like to walk |\n| Lizzy       | Has a chubby cat     |\n…renders as:\n\n\n\nStudent\nFun Fact\n\n\n\n\nJason\nDoesn’t like to walk\n\n\nLizzy\nHas a chubby cat\n\n\n\nBlock quotes can be indicated by greater-than signs (&gt;), e.g.\n&gt; Nonethless, Jason is still better at walking than Lizzy's cat.\nbecomes:\n\nNonethless, Jason is still better at walking than Lizzy’s cat.\n\nNumbered lists are equally simple, with the following chunk…\n1. Syllabus\n   -  Stern words\n2. Tease Jason\n   - Relent\n…rendering as:\n\nSyllabus\n\nStern words\n\n\nTease Jason\n\nRelent\n\n\n(Bullet points are handled as you might imagine.)\nA cheat sheet to basic and extended syntax can be found here. The web application JotBird is nice for quickly drafting Markdown documents; you may also be interested in downloading a program that can render Markdown as .pdf or .html files locally, such as Pandoc (recommended). (RMarkdown has this ability as well, though you’ll have to download LaTeX via TinyTeX, MacTeX, or another source.)"
  },
  {
    "objectID": "cli.html#directory-structure",
    "href": "cli.html#directory-structure",
    "title": "Markdown & Command Line Basics",
    "section": "",
    "text": "After opening Terminal, it’s time to get oriented and learn how to navigate a computer via the CLI. To start, we will figure out where in your file structure you actually are, using the equivalent of the R function getwd():\npwd\nEasily memorized as print working directory, this should indicate you are in your home directory (e.g., /Users/ethanlinck/), a location you can return from wherever you are by typing cd ~ and hitting enter. cd is a fundamental tool of any CLI, allowing you to change directories:\ncd / # switches to the root directory, i.e. the highest level in the hierarchical structure of file storage\ncd .. # move up one level in the file structure hierarchy, i.e. from /Users/ethanlinck/ to /Users/\ncd - # switch to the previous directory\nIt can also use explicit paths. Here is an example with a relative path (from my current home directory, /Users/ethanlinck/):\ncd teaching/genomics/\nThis would navigate to the genomics subdirectory of my teaching directory from my current location. If I am accidentally not in my home directory and there are no such folders where I am, the command will fail. An absolute path helps avoid this risk, though at the cost of flexibility:\ncd /Users/ethanlinck/teaching/genomics/\nUse cd and pwd in combination to navigate around your computer. Note that if you are correctly typing the start of a path, pressing “tab” should autocomplete it, or present options with the same suffix..\nYou can make a directory with the command mkdir and a path (including the name of the new directory):\nmkdir /teaching/genomics/\nMake a new directory entitled test/. We can then create a set of new files to put in test/ for future manipulation using echo, the assignment pipe &gt;, and a filename:\necho \"I love learning\" &gt; ~/test/text1.md; \necho \"I love computers\" &gt; ~/test/text2.md; \necho \"I regret my choices\" &gt; ~/test/.secret;\nmkdir ~/test/sub/ \n(Each line below can be entered individually; the semicolons allow you to copy and past the chunk below without manually entering linebreaks. If you misplace a semicolon or otherwise have a typo, type Ctrl+C to cancel a command. Typing exit closes the current terminal session.)\nNavigate to ~/test/. From a given location—or paired with a path (written in help documents as &lt;path&gt;) you can use the command ls to list the contents of a directory:\nls  ~/test/\nThe command ls -a will reveal “dotfiles”, typically hidden text files that begin with a period and contain information that helps software run. In this class, the dotfile .gitignore will be useful down the line; we may also manipulate .bashrc, which determines settings for the unix shell of a particular user. What are the differences between ls and ls -a when run in test/?\nAt this point you may be wondering where the argument -a came from. The following commands produce documentation (though typing an erroneous argument will also provide a brief example of proper useage)\nman lc #zsh\nlc -h #bash\nCommands can be chained together using a pipe (something you may be familiar with from R). Here I am counting the number of files in a directory using the commands ls and wc, with appropriate arguments:\nls -1 ~/test/ | wc -l # list files in a directory, 1 per line; send output to wc function, count lines\nThe command cat can be used to print (or concatenate) the contents of a file:\ncat ~/test/text1.md\nSimilarly, head can be used to print the first -n lines of a file. We will demonstrate this with a new file and the application of a double pipe (which appends text to new lines):\necho \"test\" &gt; file.md\necho \"test\" &gt;&gt; file.md\necho \"test\" &gt;&gt; file.md\necho \"test\" &gt;&gt; file.md\necho \"test\" &gt;&gt; file.md\necho \"test\" &gt;&gt; file.md\nhead -5 file.md \nThe command mv can be used to move a file to a new location, e.g:\nmv ~/test/text1.md ~/test/sub/text1.md\nThis can also be used to rename files, even within the same directory:\nmv ~/test/sub/text1.md ~/test/sub/text0.md\nAn analog is cp, which copies files:\ncp ~/test/sub/text0.md ~/test/text0.md\nUsing a . in combination with a relative or absolute path will preserve the name of the original file:\ncp ~/test/file.md ~/test/sub/. \nPasting multiple commands in a text document with the suffix .sh creates a shell script, analogous to one written in R or Python. To do so, you need a hashed line called a shebang to start your script to tell your computer to use a particular CLI. You can then add multiple commands, separated by semi-colons:\n#!/usr/bin/env bash\ncd ~/test/; \necho \"lastly\" &gt; newfile.md;\nmv newfile.md ~/test/.;\necho \"file moved\"\nSave this as script.sh in the test/ directory. To do so you may use your new plain text editor, or type nano to access a CLI-based solution available on Mac and Linux machines. (This takes a second to get used to; ask me if you need help. ) Type the command bash script.sh. Does it do what you expect?\nWildcards are characters that match multiple patterns. Most useful for our purposes is *, which will match all files in a given directory, or all\nls * # matches all files in the present directory\nls *.md # matches all files with the suffix \".md\"\nAnother useful wildcard is ?, which matches a single character:\necho \"test\" &gt; tile.md\nls ~/test/?ile.md \nLastly, the command rm removes (deletes) files and directories. For example, rm test/sub/file.md deletes the file test/sub/file.md. Paired with wildcards, you can quickly remove many files in one fell swoop.\nrm ~/test/* # removes all files, but not subdirectories\nrm -r ~/test/ # deletes the directory itself using the \"recursive\" argument -r\n\n\n\n\n\n\nWarningUsing rm with caution\n\n\n\nOne pitfall of becoming a CLI ninja is that you will not get warning messages when you inevitably deploy a powerful command somewhere you don’t intend to. rm paired with a wildcard should be used especially sparingly. What do you think would happen if you navigated to your Desktop and typed the following command?\nrm *\n\n\n\n\n\n\n\n\nNoteHomework 1\n\n\n\nWe will ease into our coding exercises with a brief shell scripting activity. I have created a set of dummy data files that are typical of population genomics—.fastq.gz raw sequencing read files, .fasta assembled sequence data, text files (*.txt), and metadata (*.csv) files. First, download these data using the command curl:\ncurl -L -O https://raw.githubusercontent.com/elinck/genomics_eco_con/main/data/week_1.tar.gz -o week_1.tar.gz # download a resource at a url\ntar -xzf week_1.tar.gz # this command unzips a \"tarball\"\ncd week_1 # enter the data directory\nUsing the information above and any other resource available to you*, write a single bash script that can perform the following steps:\nWrite a bash script called 01_homework_&lt;your_name&gt;.sh that does the following:\n\nChange into the week_1 directory;\nCreate three subdirectories called:\n\n\nfastq/\nfasta/\nmetadata/\n\n\nMove files into these directories based on file extension:\n\n\n*.fastq.gz to fastq/\n*.fasta to fasta/\n*.csv to metadata/\n\n\nCount how many files are in each new directory and print to the screen.\n\nFor now, you may keep this in your local directory; we’ll work on submitting it as part of next week’s activities.\n* This will generally include AI tools, and as these are an indispensible part of modern programming, I am hesitant to outright ban them for this course. However, it will be in your best interest to attempt to put this script together yourself—from its component parts—as these commands need to become second nature as you navigate the cluster."
  },
  {
    "objectID": "git.html",
    "href": "git.html",
    "title": "Version Control with Git and GitHub",
    "section": "",
    "text": "Our discussion of reproducibility and its importance to scientific programming and bioinformatics touched on the challenge of keeping track of the “final” version of an analysis in the face of data exploration and troubleshooting. Consider your own history of writing code for data manipulation and analysis. What strategies have you used to keep track of multiple versions of an analysis? Common approaches include saving multiple versions of files with different names, repeatedly pasting the same chunk of code with slight modifications in the same script, and closed-source, cloud-based file storage solutions like Dropbox and Google Drive. The latter provide some degree of version control, i.e. snapshots of the version of a file at a particular time point. But they suffer from many of the disadvantages of proprietary software, and are not optimized for writing computer code, plain text files, or working collaboratively.\nSoftware developers have created a powerful solution to this common dilemma, with clear benefits to conducting reproducible research in ecology, evolution, and related fields. Pairing a version control system (VCS; local software) with online hosting sites (to distribute and back up file versions) allows continuous, collaborative editing across dispersed contributers, annotation of different versions of files, and an easy way to share open-source scientific software and documents for review and to aid other researchers. Though the learning curve can be steep, learning to effectively use a VCS and online repository is a minimum standard for conducting ethical, transparent computational research—especially when working with genomic data. It also will open the door to using myriad task-specific software packages written by the vibrant scientific computing community. In this class, we will learn the basics of Git and the online hosting service GitHub, though the principles discussed apply to other platforms.\n\n\nIf you have not yet worked with Git and GitHub—or have not yet downloaded Git for Windows to obtain a bash emulator—you will likely need to install the appropriate version of Git for your operating system. To do so, follow the instructions on this website. Once you have completed the installation, open your terminal and type the following command:\ngit\nAssuming the output is a lengthy message of command options, you have successfully installed Git.\n\n\n\nAfter installing Git, you’ll need a GitHub account to link to it (assuming you don’t already have one). To do so, click “Sign Up” on the website; detailed instructions are available here. For better or worse, you will also have to activate two-factor authentication (2FA) in some way. Make sure you verify your email address.\nNext, you’ll need to tell Git who you are, using the exact user.name and user.email previously provided to GitHub:\ngit config --global user.name \"Jean-Baptiste Lamarck\"\ngit config --global user.email \"lamarck@mnhn.fr\"\nVerify your information has been entered correctly with the following command:\ngit config --global --list\nAfter this, you’ll need to cache your credentials to avoid being prompted to enter a username and password everytime you interact with a remote repository. This is one of the trickier steps to getting Git and GitHub playing nicely together, and I anticipate some of you will have issues.\n\nTo start, navigate to Settings → Developer Settings.\nClick “Tokens (classic)” under “Personal access tokens”.\nClick “Generate new token” and select “Generate new token (classic)”.\nIn the “Note” field, type something informative, avoiding spaces (e.g., “bioe591-token”).\nSelect an expiration date of June 2026 or later.\nUnder “Select scopes”, click the main “repo” box, and then “Generate token”.\nIMMEDIATELY save the text string that appears to a single line of a text editor—this will quickly disappear, and you’ll need to start over.\n\nOnce you’ve created a PAT, you’ll want to save it on your computer’s credentialing system. The exact steps will differ by operating system.\nMac OS: Type the following command:\ngit config --global credential.helper osxkeychain\nNext, type a command that requires a link between Git and GitHub:\ngit ls-remote https://github.com/USERNAME/this-repo-does-not-exist\nYou should be prompted to enter your username and password. When the “password” prompt appears in the terminal, paste the entire PAT string from your plain text document into the field. You will NOT see anything appear; this is normal. Hit enter; the command will then indicate the repository does not exist (this is expected). You should now be all set to use Git and GitHub without repeated authentication requests.\nWindows: Type the following command:\ngit config --global --get credential.helper\nLikely output will be manager. If this does not appear, configure it explicitly:\ngit config --global credential.helper manager\nYou will then need type a command that requires a link between Git and GitHub:\ngit ls-remote https://github.com/USERNAME/this-repo-does-not-exist\nAs with Mac OS, this will prompt you to enter your username, then your password; you may paste the PAT string in the password field and hit enter. It will save to your computer’s credential manager automatically.\nLinux: Enter the following commands:\ngit config --global credential.helper store\nchmod 600 ~/.git-credentials # ensure you have appropriate permissions to write to this file\nAs with the other operating systems, this will store your PAT after your are prompted to enter it:\ngit ls-remote https://github.com/USERNAME/this-repo-does-not-exist\n(Unlike the other OS, it will be in an easily-found dotfile: ~/.git-credentials. We will use a separate authentication protocol when working from the cluster, to be introduced later.)\n\n\n\nWith a bit of luck, the previous steps should have been relatively painless and you should now be ready to begin your first Git and GitHub workflow. To start, navigate to GitHub and create a repository (via the “Repositories” tab and then the “New” button). Name your repository something associated with this class and your initials, or an otherwise informative and unique combination. You may also enter a brief description of the contents of the repository in the provided bar. Leave all other options blank for now. The new repository should now open as a stand-alone webpage. Under the green “&lt;&gt; Code” button, copy the link under the HTTPS tag. Open your terminal, navigate to a directory where you keep homework or research projects, and make a new directory for this class:\nmkdir bioe591_work\nYou next want to initialize this as Git repository:\ngit init\nThis command will then create a new dotfile (i.e., a hidden file, technically a directory) with the name of .git. Its contents can be made visible with the following command, though you can ignore them now and always:\nls .git\nYou will then link your local repository to its online counterpart. In the command below, origin is an alias for the URL that Git is assigning to be a remote copy of your local work. (You’ll want to replace the link below with the link to your new GitHub repository, of course.)\ngit remote add origin https://github.com/USERNAME/class-repo\nOnce this is done, it’s time to test out a sample workflow. We can start by creating a file called README.md, which by default will be displayed on as the landing page for your GitHub repository, complete with Markdown formatting. For the purposes of the tutorial, we can create this with a single line of code:\necho \"# BIOE591 Coursework Repository\" &gt; README.md\n(However, you should feel free to edit this as you see fit, either with nano or another text editor of your choice—for now, the contents don’t matter.)\nNext, we’ll create a special dotfile called .gitignore. This is not strictly necessary, but can be helpful if you want to avoid adding particular files to your remote repository. Consider a scenario in which you have a folder filled with sequencing reads (raw_reads/). These data are large and cumbersome, and you won’t want to hsot them on GitHub itself. Add them to your .gitignore with the following command:\necho \"raw_reads/\" &gt; .gitignore\nA second line can be added to the same file using &gt;&gt;. On Mac OS systems, for example, a pesky dotfile called .DS_Store is often present, and never needed in a repository. Just to be sure (and as an exercise regardless of your operating system), let’s add it as well, and then print the contents of the file to the screen:\necho \". DS_Store\" &gt;&gt; .gitignore\ncat .gitignore\nWe are now ready to update our remote repository. To do so, we will take a series of steps that 1) tell Git which files to track (git add); 2) move a snapshot of the version of the files from our current working directory to a staging area (git commit); 3) upload these file version to GitHub (git push). Let’s work through these one by one. To begin, we need to begin tracking the contents of the repository. The quickest way to do this is to type the following command, where . indicates the entire contents of the directory:\ngit add .\nThis is exactly equivalent to typing git add README.md followed by git add .gitignore, i.e. manually tracking both files (a move which can sometimes be useful). By default, git add will only impact files with changed contents (including brand new files). Next, you need move these tracked files to a staging area, where you will add a short note (using the -m flag) describing the reason for any revisions, deletions, or additions:\ngit commit -m \"add first files\" \nAt this point it is time for a brief digression. We have already discussed the difference between your working directory, an abstracted “staging area”, and your remote repository. For the vast majority of work you are likely to do, waiting until code is ostensible bug-free to add, commit, and push changes within your working directory to a remote repository is likely to be best practice. For more complex projects, however, it can be useful to have a local copy of your code to modify and experiment with while keeping the last working version in safe condition. Git’s solution to this is called branching. By default, the first version of a particular project is assigned to a branch called main, which is initiated with your first commit. You can verify this by typing the following command:\ngit branch\nA suite of commands are associated with creating, switching between, and ultimately merging work in different branches. These are beyond the scope of our introduction, but if you are interested in a deeper understanding, I recommend visiting Git’s tutorial on the topic. For now, it will suffice to know that your work in this class will always be in main.\n\n\n\nA flow chart of fundamental Git commands and their relationship to working directories and repositories, courtesy of Hirdyansh Pandey on LinkedIn\n\n\nWe now return to our workflow. Recall that we have “committed” a set of files (README.md and .gitignore) to a staging area, with a message describing our work. We are now ready to add these files to our online repository:\ngit push -u origin main\nHere, the command -u sets “upstream tracking”—a fancy way of saying your local changes will always get sent to the online repository when you push, and not anywhere else. (Practically, this means you can type git push without the rest of the command whenever you work in this repository from now on.) You can now navigate the the url for your remote repository. Unless something has gone wrong, you should see it has been updated to reflect the tracked, commited contents of your local directory. Congratulations! You’ve now effectively used Git and GitHub. Most of the time, this is all there is to it. However, it will be helpful to address two other common uses of version control before concluding. First, let’s learn how to revert to an earlier version of a particular project. To do so, we’ll edit our README, commit it, and push it again:\necho \"questionable edit\" &gt;&gt; README.md\ngit add README.md\ngit commit -m \"edit I will not regret \"\ngit push\nYour GitHub repository should now show a modified README.md file. Unfortunately, this tweak breaks your data analysis pipeline, and you decide you need to return to the script you were working with earlier. To do so, click the link that reads “2 Commits” below the green “Code” button. Here, you will see your full commit history: two file versions, each with a unique code in a format similar to b38e27a. (This same history can be shown locally with the command git log --oneline.) The command git checkout can be paired with a commit code to instantly revert to that snapshot of your work:\ngit checkout c66e60b -- .\n(The command -- is a delimiter telling Git that everything afterwards should not be interpreted as a command; the . is a wildcard indicating that you wish to change the entire contents of the directory.).\nTyping git status should show that you now have changes waiting to be committed. Add, commit, and push this reversion, then confirm that your GitHub repository has been updated appropriately.\nOur final Git lesson involves deliberately breaking something, which we do with the hope that it will help you troubleshoot accidental errors in the future. Back on your GitHub repository, navigate to README.md and click the edit button (a sketch of a pencil on the righthand side of the document when seen from the landing page). Doing so opens a text editor. In the contents of the file, add a line reading “github edit” or similar. You will be prompted to commit this edit—go ahead and do so.\nNext, return to your working directory. Using nano or another text editor, edit your local copy of README.md, adding the line “local edit”. Save, add, and commit. Now, attempt to push your work to GitHub (git push). You should see an error message that reads Updates were rejected because the remote contains work that you do not have locally. This is usually caused by another repository pushing to the same ref. You may want to first integrate the remote changes (e.g., 'git pull ...') before pushing again.\nIn general, following the advice provided by error messages is solid practice. Type git pull. The following text should appear:\nAuto-merging README.md  \nCONFLICT (content): Merge conflict in README.md  \nAutomatic merge failed; fix conflicts and then commit the result.  \nThis is a common sticking point and can be one of the most difficult aspects of working with a VCS for beginners. Luckily, the solution is relatively simple. Using nano or another editor, open the README.md file. You’ll see text similar to the following:\n&lt;&lt;&lt;&lt;&lt;&lt;&lt; HEAD\nlocal edit\n=======\ngithub edit\n&gt;&gt;&gt;&gt;&gt;&gt;&gt; origin/main\nThe arrows (&gt;&gt;&gt;) and equals signs (===) have been added by Git to highlight a conflict between the two versions. All the software is doing is prompting you to choose between the two file versions (or combine them). Edit as you see fit—which typically involves removing those delimiters—add, commit, and push. Crisis averted!\nFinally, it can be useful to visualize exactly how your working directory and remote repository have changed and merged over time. To do so, use the following command:\ngit log --oneline --graph\n\n\n\nYou should now feel comfortable—well, at least understand the purpose, in the abstract—of the following commands:\ngit status\ngit add\ngit commit\ngit pull\ngit push\ngit log --oneline --graph\ngit checkout -b\nWith time, these should become second nature, and help you develop efficient, safe, and reproducible workflows.\n\n\n\n\n\n\nNoteHomework 2\n\n\n\nToday’s homework is fairly simple, and intended to reinforce the skills you have just built:\n\nIn the repository you made for this class, create a new subdirectory called homework/. Move your bash script from last week (01_homework_&lt;your_name&gt;.sh) inside it.\n\nEdit your README.md file to describe the overall structure of your repository and its subdirectories. This file will be my guide to finding to your homework assignments, so the more informative, the better!\nAdd, commit, and push the script to GitHub.\nReturn to 01_homework_&lt;your_name&gt;.sh. Add a line of bash code to the end of the script that prints “DONE!” or something similar to the screen after counting all files in the directory. Add, commit, and push this new version to GitHub. (I will be checking the commit log on your account, so make sure you add a message signaling this change.)"
  },
  {
    "objectID": "git.html#installation",
    "href": "git.html#installation",
    "title": "Version Control with Git and GitHub",
    "section": "",
    "text": "If you have not yet worked with Git and GitHub—or have not yet downloaded Git for Windows to obtain a bash emulator—you will likely need to install the appropriate version of Git for your operating system. To do so, follow the instructions on this website. Once you have completed the installation, open your terminal and type the following command:\ngit\nAssuming the output is a lengthy message of command options, you have successfully installed Git."
  },
  {
    "objectID": "git.html#linking-git-and-github",
    "href": "git.html#linking-git-and-github",
    "title": "Version Control with Git and GitHub",
    "section": "",
    "text": "After installing Git, you’ll need a GitHub account to link to it (assuming you don’t already have one). To do so, click “Sign Up” on the website; detailed instructions are available here. For better or worse, you will also have to activate two-factor authentication (2FA) in some way. Make sure you verify your email address.\nNext, you’ll need to tell Git who you are, using the exact user.name and user.email previously provided to GitHub:\ngit config --global user.name \"Jean-Baptiste Lamarck\"\ngit config --global user.email \"lamarck@mnhn.fr\"\nVerify your information has been entered correctly with the following command:\ngit config --global --list\nAfter this, you’ll need to cache your credentials to avoid being prompted to enter a username and password everytime you interact with a remote repository. This is one of the trickier steps to getting Git and GitHub playing nicely together, and I anticipate some of you will have issues.\n\nTo start, navigate to Settings → Developer Settings.\nClick “Tokens (classic)” under “Personal access tokens”.\nClick “Generate new token” and select “Generate new token (classic)”.\nIn the “Note” field, type something informative, avoiding spaces (e.g., “bioe591-token”).\nSelect an expiration date of June 2026 or later.\nUnder “Select scopes”, click the main “repo” box, and then “Generate token”.\nIMMEDIATELY save the text string that appears to a single line of a text editor—this will quickly disappear, and you’ll need to start over.\n\nOnce you’ve created a PAT, you’ll want to save it on your computer’s credentialing system. The exact steps will differ by operating system.\nMac OS: Type the following command:\ngit config --global credential.helper osxkeychain\nNext, type a command that requires a link between Git and GitHub:\ngit ls-remote https://github.com/USERNAME/this-repo-does-not-exist\nYou should be prompted to enter your username and password. When the “password” prompt appears in the terminal, paste the entire PAT string from your plain text document into the field. You will NOT see anything appear; this is normal. Hit enter; the command will then indicate the repository does not exist (this is expected). You should now be all set to use Git and GitHub without repeated authentication requests.\nWindows: Type the following command:\ngit config --global --get credential.helper\nLikely output will be manager. If this does not appear, configure it explicitly:\ngit config --global credential.helper manager\nYou will then need type a command that requires a link between Git and GitHub:\ngit ls-remote https://github.com/USERNAME/this-repo-does-not-exist\nAs with Mac OS, this will prompt you to enter your username, then your password; you may paste the PAT string in the password field and hit enter. It will save to your computer’s credential manager automatically.\nLinux: Enter the following commands:\ngit config --global credential.helper store\nchmod 600 ~/.git-credentials # ensure you have appropriate permissions to write to this file\nAs with the other operating systems, this will store your PAT after your are prompted to enter it:\ngit ls-remote https://github.com/USERNAME/this-repo-does-not-exist\n(Unlike the other OS, it will be in an easily-found dotfile: ~/.git-credentials. We will use a separate authentication protocol when working from the cluster, to be introduced later.)"
  },
  {
    "objectID": "git.html#git-and-github-basics",
    "href": "git.html#git-and-github-basics",
    "title": "Version Control with Git and GitHub",
    "section": "",
    "text": "With a bit of luck, the previous steps should have been relatively painless and you should now be ready to begin your first Git and GitHub workflow. To start, navigate to GitHub and create a repository (via the “Repositories” tab and then the “New” button). Name your repository something associated with this class and your initials, or an otherwise informative and unique combination. You may also enter a brief description of the contents of the repository in the provided bar. Leave all other options blank for now. The new repository should now open as a stand-alone webpage. Under the green “&lt;&gt; Code” button, copy the link under the HTTPS tag. Open your terminal, navigate to a directory where you keep homework or research projects, and make a new directory for this class:\nmkdir bioe591_work\nYou next want to initialize this as Git repository:\ngit init\nThis command will then create a new dotfile (i.e., a hidden file, technically a directory) with the name of .git. Its contents can be made visible with the following command, though you can ignore them now and always:\nls .git\nYou will then link your local repository to its online counterpart. In the command below, origin is an alias for the URL that Git is assigning to be a remote copy of your local work. (You’ll want to replace the link below with the link to your new GitHub repository, of course.)\ngit remote add origin https://github.com/USERNAME/class-repo\nOnce this is done, it’s time to test out a sample workflow. We can start by creating a file called README.md, which by default will be displayed on as the landing page for your GitHub repository, complete with Markdown formatting. For the purposes of the tutorial, we can create this with a single line of code:\necho \"# BIOE591 Coursework Repository\" &gt; README.md\n(However, you should feel free to edit this as you see fit, either with nano or another text editor of your choice—for now, the contents don’t matter.)\nNext, we’ll create a special dotfile called .gitignore. This is not strictly necessary, but can be helpful if you want to avoid adding particular files to your remote repository. Consider a scenario in which you have a folder filled with sequencing reads (raw_reads/). These data are large and cumbersome, and you won’t want to hsot them on GitHub itself. Add them to your .gitignore with the following command:\necho \"raw_reads/\" &gt; .gitignore\nA second line can be added to the same file using &gt;&gt;. On Mac OS systems, for example, a pesky dotfile called .DS_Store is often present, and never needed in a repository. Just to be sure (and as an exercise regardless of your operating system), let’s add it as well, and then print the contents of the file to the screen:\necho \". DS_Store\" &gt;&gt; .gitignore\ncat .gitignore\nWe are now ready to update our remote repository. To do so, we will take a series of steps that 1) tell Git which files to track (git add); 2) move a snapshot of the version of the files from our current working directory to a staging area (git commit); 3) upload these file version to GitHub (git push). Let’s work through these one by one. To begin, we need to begin tracking the contents of the repository. The quickest way to do this is to type the following command, where . indicates the entire contents of the directory:\ngit add .\nThis is exactly equivalent to typing git add README.md followed by git add .gitignore, i.e. manually tracking both files (a move which can sometimes be useful). By default, git add will only impact files with changed contents (including brand new files). Next, you need move these tracked files to a staging area, where you will add a short note (using the -m flag) describing the reason for any revisions, deletions, or additions:\ngit commit -m \"add first files\" \nAt this point it is time for a brief digression. We have already discussed the difference between your working directory, an abstracted “staging area”, and your remote repository. For the vast majority of work you are likely to do, waiting until code is ostensible bug-free to add, commit, and push changes within your working directory to a remote repository is likely to be best practice. For more complex projects, however, it can be useful to have a local copy of your code to modify and experiment with while keeping the last working version in safe condition. Git’s solution to this is called branching. By default, the first version of a particular project is assigned to a branch called main, which is initiated with your first commit. You can verify this by typing the following command:\ngit branch\nA suite of commands are associated with creating, switching between, and ultimately merging work in different branches. These are beyond the scope of our introduction, but if you are interested in a deeper understanding, I recommend visiting Git’s tutorial on the topic. For now, it will suffice to know that your work in this class will always be in main.\n\n\n\nA flow chart of fundamental Git commands and their relationship to working directories and repositories, courtesy of Hirdyansh Pandey on LinkedIn\n\n\nWe now return to our workflow. Recall that we have “committed” a set of files (README.md and .gitignore) to a staging area, with a message describing our work. We are now ready to add these files to our online repository:\ngit push -u origin main\nHere, the command -u sets “upstream tracking”—a fancy way of saying your local changes will always get sent to the online repository when you push, and not anywhere else. (Practically, this means you can type git push without the rest of the command whenever you work in this repository from now on.) You can now navigate the the url for your remote repository. Unless something has gone wrong, you should see it has been updated to reflect the tracked, commited contents of your local directory. Congratulations! You’ve now effectively used Git and GitHub. Most of the time, this is all there is to it. However, it will be helpful to address two other common uses of version control before concluding. First, let’s learn how to revert to an earlier version of a particular project. To do so, we’ll edit our README, commit it, and push it again:\necho \"questionable edit\" &gt;&gt; README.md\ngit add README.md\ngit commit -m \"edit I will not regret \"\ngit push\nYour GitHub repository should now show a modified README.md file. Unfortunately, this tweak breaks your data analysis pipeline, and you decide you need to return to the script you were working with earlier. To do so, click the link that reads “2 Commits” below the green “Code” button. Here, you will see your full commit history: two file versions, each with a unique code in a format similar to b38e27a. (This same history can be shown locally with the command git log --oneline.) The command git checkout can be paired with a commit code to instantly revert to that snapshot of your work:\ngit checkout c66e60b -- .\n(The command -- is a delimiter telling Git that everything afterwards should not be interpreted as a command; the . is a wildcard indicating that you wish to change the entire contents of the directory.).\nTyping git status should show that you now have changes waiting to be committed. Add, commit, and push this reversion, then confirm that your GitHub repository has been updated appropriately.\nOur final Git lesson involves deliberately breaking something, which we do with the hope that it will help you troubleshoot accidental errors in the future. Back on your GitHub repository, navigate to README.md and click the edit button (a sketch of a pencil on the righthand side of the document when seen from the landing page). Doing so opens a text editor. In the contents of the file, add a line reading “github edit” or similar. You will be prompted to commit this edit—go ahead and do so.\nNext, return to your working directory. Using nano or another text editor, edit your local copy of README.md, adding the line “local edit”. Save, add, and commit. Now, attempt to push your work to GitHub (git push). You should see an error message that reads Updates were rejected because the remote contains work that you do not have locally. This is usually caused by another repository pushing to the same ref. You may want to first integrate the remote changes (e.g., 'git pull ...') before pushing again.\nIn general, following the advice provided by error messages is solid practice. Type git pull. The following text should appear:\nAuto-merging README.md  \nCONFLICT (content): Merge conflict in README.md  \nAutomatic merge failed; fix conflicts and then commit the result.  \nThis is a common sticking point and can be one of the most difficult aspects of working with a VCS for beginners. Luckily, the solution is relatively simple. Using nano or another editor, open the README.md file. You’ll see text similar to the following:\n&lt;&lt;&lt;&lt;&lt;&lt;&lt; HEAD\nlocal edit\n=======\ngithub edit\n&gt;&gt;&gt;&gt;&gt;&gt;&gt; origin/main\nThe arrows (&gt;&gt;&gt;) and equals signs (===) have been added by Git to highlight a conflict between the two versions. All the software is doing is prompting you to choose between the two file versions (or combine them). Edit as you see fit—which typically involves removing those delimiters—add, commit, and push. Crisis averted!\nFinally, it can be useful to visualize exactly how your working directory and remote repository have changed and merged over time. To do so, use the following command:\ngit log --oneline --graph"
  },
  {
    "objectID": "git.html#summary",
    "href": "git.html#summary",
    "title": "Version Control with Git and GitHub",
    "section": "",
    "text": "You should now feel comfortable—well, at least understand the purpose, in the abstract—of the following commands:\ngit status\ngit add\ngit commit\ngit pull\ngit push\ngit log --oneline --graph\ngit checkout -b\nWith time, these should become second nature, and help you develop efficient, safe, and reproducible workflows.\n\n\n\n\n\n\nNoteHomework 2\n\n\n\nToday’s homework is fairly simple, and intended to reinforce the skills you have just built:\n\nIn the repository you made for this class, create a new subdirectory called homework/. Move your bash script from last week (01_homework_&lt;your_name&gt;.sh) inside it.\n\nEdit your README.md file to describe the overall structure of your repository and its subdirectories. This file will be my guide to finding to your homework assignments, so the more informative, the better!\nAdd, commit, and push the script to GitHub.\nReturn to 01_homework_&lt;your_name&gt;.sh. Add a line of bash code to the end of the script that prints “DONE!” or something similar to the screen after counting all files in the directory. Add, commit, and push this new version to GitHub. (I will be checking the commit log on your account, so make sure you add a message signaling this change.)"
  },
  {
    "objectID": "reads.html",
    "href": "reads.html",
    "title": "Understanding .fastq Data and Short Read Quality Control",
    "section": "",
    "text": "Our introduction to the Tempest HPC included our first exposure to real live short read sequencing data. I will assume that this week’s reading and discussion has everyone feeling a little more comfortable with how these data are generated and what a read is in the first place. Today’s lab, which is somewhat shorter than its predecessors, will cover the basics of .fastq data: its format, what aspects of a sample and its preparation are contained in reads, and how to trim and filter reads to ensure that only high-quality data get passed on to the next stage of your workflow.\n\n\nThe remaining classes before spring break will lead us through an abbreviated sequence alignment, variant calling, and variant filtering pipeline. These basic bioinformatic processing steps are required for nearly all population-level genomics and transcriptomics projects; while the exact programs and filtering choices will vary, the backbone presented here should orient you to successfully preparing your own data for subsequent analysis. To achieve this goal, we will work with unpublished exon capture data from 13 species of high Andean tanagers (Passeriformes: Thraupidae). These data were generated by exon capture—a kind of reduced representation library preparation method that targets specific expressed genes by enriches them with synthetic user-designed probes—of the seven hemoglobin peptide chains, or subunits. Because hemoglobin structure impacts its ability to bind and transport \\(O_2\\) in the blood, variation in underlying sequences may be associated with adaptations to high or low altitude and attendant difference in the partial pressure of oxygen (\\(PO_2\\)).\nBelow is a complete list of species with linked natural history and distributional data. Feel free to pick your favorite, but note that sample sizes vary, posing a trade-off between the time needed to run tools and how interesting the data ultimately is. All can be found in named subdirectories within bioe-591-genomics/course-materials/data/raw_reads/.\n\nGiant Conebill Oreomanes fraseri (outdated binomial used here)\nCapped Conebill Conirostrum albifrons\nCinereous Conebill Conirostrum cinereum\nBlue-backed Conebill Conirostrum sitticolor\nWhite-sided Flowerpiercer Diglossa albilatera\nCinnamon-bellied Flowerpiercer Diglossa baritula\nBlack-throated Flowerpiercer Diglossa brunneiventris\nBlueish Flowerpiercer Diglossa caerulescens\nMasked Flowerpiercer Diglossa cyanea\nDeep-blue Flowerpiercer Diglossa glauca\nGlossy Flowerpiercer Diglossa lafresnayii\nMoustached Flowerpiercer Diglossa mystacalis\nRusty Flowerpiercer Diglossa sittoides\n\n\n\n\nSequencing reads are turned into plain-text data as .fastq files, which are then typically compressed using gzip into compressed .fast.gz files. Typing gunzip filename.fastq.gz will decompress the target, albeit slowly; because most tools can read compressed data and even small genomic datasets can be prohibitively large in their raw form, there is usually little reason to do this. However, it can be useful to look at least part of a human-readable .fastq. Luckily, we can do so using a version of the familiar bash command cat that works on compressed data (zcat). We’ll pipe this to head to avoid printing the entire contents of the file to your screen; if you’d like to expand beyond ten lines, use head’s -n &lt;integer&gt; argument.\nzcat intro/catamenia_analis.fastq.gz | head\nThe output of this command should look something like this:\n@J00138:88:HG7TGBBXX:3:1101:1083:1086 1:N:0:NGCGGAAT+NATGTTCC\nNTCAAAGTCCAGCCCAGACCTCTCACCTTGCAGACAACTCCTTCCCCAACACGGCCCCCCACACCCCGCCCCCCCCAACCCCCCCCCAAACCCCCCAACAGCCCCCCCCGGCCCCCCCCTGCTCCCGCCCCCCCCCCCCGGCTCCCCCCCC\n+\n#&lt;AAFJFAFFJ77JJ&lt;JJJJJJJ&lt;JJJJJJJ7J7AJ7JJJJAF&lt;FAA--F-7--7-AAJ----777-7A-7FJA----777JJ----7--7-AJF-------7-7AA----7-FF7A77-----------A-A)-A)-)))))))-A---)\n@J00138:88:HG7TGBBXX:3:1101:7415:1086 1:N:0:NGCGGAAT+NATGTTCC\nNTCAGGGCATGGGCAAATGCATAGACTGGCACAGACCCTCCCCAGAACTGGGAATTCAGGGCACGGGCAAATGCACAGACTGGCACAGACCCTCCCCAGAACTGGGAATTCAGGGCATGGGCAAATGCACAGACTGGCACAGACCCTCCCC\n+\n#AAAFFAJJJJJJJFJJJJJJJJJJJJJFFJJJJJJJJFJJJJJJJJJJJJJJJJJJJJJFAJJJAJJJJJJFJJFJFJJJJJJJFJA&lt;JAJJJJJJJJFJJJJJFJFFJFJJFJF-AFFJAFJFFJJJA-&lt;AFJ-FAFFJJA7-)&lt;A&lt;AJ\n@J00138:88:HG7TGBBXX:3:1101:8958:1086 1:N:0:NGCGGAAT+NATGTTCC\nNGTGAGCGAGCAACAGCGCCAGAGGGCAGACAGGATGGAAGAGCGCAAGAACAGAGCCTGGGGAGTGCGGAGACGCCGGCAGGCAGTTCACTTCCCATGCTCTTTCTTCTCCTGTGTGTCGTGACAGCAGCAGCAGCAGCAGCAGCAGCAG\nThough initially overwhelming, you will notice that the characters above consist of only four lines, which can be interpreted as follows:\n\na unique sequence identifier;\nthe nucleotide sequence itself;\na quality score identifier line (simply a +)\na quality score, encoded by an ASCII character.\n\nThe sequence identifier, which is initially cryptic, follows the format below. Note that these are technical details about sequencing itself, and largely not the biology of the sample.\n@&lt;instrument&gt;:&lt;run number&gt;:&lt;flowcell ID&gt;:&lt;lane&gt;:&lt;tile&gt;:&lt;x-pos&gt;:&lt;y-pos&gt; &lt;read&gt;:&lt;is filtered&gt;:&lt;control number&gt;:&lt;sample number&gt;\nThe nucleotide sequence itself should be self explanatory, though note that N (and sometimes other )\n(Illumina has a helpful explainer on exactly what each character preceding the sequence data means.)\nIn your own subdirectory, dig up the size.txt file you generated during last week’s activity.\nreads   bases\n5417329 818016679"
  },
  {
    "objectID": "reads.html#example-data",
    "href": "reads.html#example-data",
    "title": "Understanding .fastq Data and Short Read Quality Control",
    "section": "",
    "text": "The remaining classes before spring break will lead us through an abbreviated sequence alignment, variant calling, and variant filtering pipeline. These basic bioinformatic processing steps are required for nearly all population-level genomics and transcriptomics projects; while the exact programs and filtering choices will vary, the backbone presented here should orient you to successfully preparing your own data for subsequent analysis. To achieve this goal, we will work with unpublished exon capture data from 13 species of high Andean tanagers (Passeriformes: Thraupidae). These data were generated by exon capture—a kind of reduced representation library preparation method that targets specific expressed genes by enriches them with synthetic user-designed probes—of the seven hemoglobin peptide chains, or subunits. Because hemoglobin structure impacts its ability to bind and transport \\(O_2\\) in the blood, variation in underlying sequences may be associated with adaptations to high or low altitude and attendant difference in the partial pressure of oxygen (\\(PO_2\\)).\nBelow is a complete list of species with linked natural history and distributional data. Feel free to pick your favorite, but note that sample sizes vary, posing a trade-off between the time needed to run tools and how interesting the data ultimately is. All can be found in named subdirectories within bioe-591-genomics/course-materials/data/raw_reads/.\n\nGiant Conebill Oreomanes fraseri (outdated binomial used here)\nCapped Conebill Conirostrum albifrons\nCinereous Conebill Conirostrum cinereum\nBlue-backed Conebill Conirostrum sitticolor\nWhite-sided Flowerpiercer Diglossa albilatera\nCinnamon-bellied Flowerpiercer Diglossa baritula\nBlack-throated Flowerpiercer Diglossa brunneiventris\nBlueish Flowerpiercer Diglossa caerulescens\nMasked Flowerpiercer Diglossa cyanea\nDeep-blue Flowerpiercer Diglossa glauca\nGlossy Flowerpiercer Diglossa lafresnayii\nMoustached Flowerpiercer Diglossa mystacalis\nRusty Flowerpiercer Diglossa sittoides"
  },
  {
    "objectID": "reads.html#the-.fastq-format",
    "href": "reads.html#the-.fastq-format",
    "title": "Understanding .fastq Data and Short Read Quality Control",
    "section": "",
    "text": "Sequencing reads are turned into plain-text data as .fastq files, which are then typically compressed using gzip into compressed .fast.gz files. Typing gunzip filename.fastq.gz will decompress the target, albeit slowly; because most tools can read compressed data and even small genomic datasets can be prohibitively large in their raw form, there is usually little reason to do this. However, it can be useful to look at least part of a human-readable .fastq. Luckily, we can do so using a version of the familiar bash command cat that works on compressed data (zcat). We’ll pipe this to head to avoid printing the entire contents of the file to your screen; if you’d like to expand beyond ten lines, use head’s -n &lt;integer&gt; argument.\nzcat intro/catamenia_analis.fastq.gz | head\nThe output of this command should look something like this:\n@J00138:88:HG7TGBBXX:3:1101:1083:1086 1:N:0:NGCGGAAT+NATGTTCC\nNTCAAAGTCCAGCCCAGACCTCTCACCTTGCAGACAACTCCTTCCCCAACACGGCCCCCCACACCCCGCCCCCCCCAACCCCCCCCCAAACCCCCCAACAGCCCCCCCCGGCCCCCCCCTGCTCCCGCCCCCCCCCCCCGGCTCCCCCCCC\n+\n#&lt;AAFJFAFFJ77JJ&lt;JJJJJJJ&lt;JJJJJJJ7J7AJ7JJJJAF&lt;FAA--F-7--7-AAJ----777-7A-7FJA----777JJ----7--7-AJF-------7-7AA----7-FF7A77-----------A-A)-A)-)))))))-A---)\n@J00138:88:HG7TGBBXX:3:1101:7415:1086 1:N:0:NGCGGAAT+NATGTTCC\nNTCAGGGCATGGGCAAATGCATAGACTGGCACAGACCCTCCCCAGAACTGGGAATTCAGGGCACGGGCAAATGCACAGACTGGCACAGACCCTCCCCAGAACTGGGAATTCAGGGCATGGGCAAATGCACAGACTGGCACAGACCCTCCCC\n+\n#AAAFFAJJJJJJJFJJJJJJJJJJJJJFFJJJJJJJJFJJJJJJJJJJJJJJJJJJJJJFAJJJAJJJJJJFJJFJFJJJJJJJFJA&lt;JAJJJJJJJJFJJJJJFJFFJFJJFJF-AFFJAFJFFJJJA-&lt;AFJ-FAFFJJA7-)&lt;A&lt;AJ\n@J00138:88:HG7TGBBXX:3:1101:8958:1086 1:N:0:NGCGGAAT+NATGTTCC\nNGTGAGCGAGCAACAGCGCCAGAGGGCAGACAGGATGGAAGAGCGCAAGAACAGAGCCTGGGGAGTGCGGAGACGCCGGCAGGCAGTTCACTTCCCATGCTCTTTCTTCTCCTGTGTGTCGTGACAGCAGCAGCAGCAGCAGCAGCAGCAG\nThough initially overwhelming, you will notice that the characters above consist of only four lines, which can be interpreted as follows:\n\na unique sequence identifier;\nthe nucleotide sequence itself;\na quality score identifier line (simply a +)\na quality score, encoded by an ASCII character.\n\nThe sequence identifier, which is initially cryptic, follows the format below. Note that these are technical details about sequencing itself, and largely not the biology of the sample.\n@&lt;instrument&gt;:&lt;run number&gt;:&lt;flowcell ID&gt;:&lt;lane&gt;:&lt;tile&gt;:&lt;x-pos&gt;:&lt;y-pos&gt; &lt;read&gt;:&lt;is filtered&gt;:&lt;control number&gt;:&lt;sample number&gt;\nThe nucleotide sequence itself should be self explanatory, though note that N (and sometimes other )\n(Illumina has a helpful explainer on exactly what each character preceding the sequence data means.)\nIn your own subdirectory, dig up the size.txt file you generated during last week’s activity.\nreads   bases\n5417329 818016679"
  },
  {
    "objectID": "assessment.html",
    "href": "assessment.html",
    "title": "BIOE591: Preliminary Assessment",
    "section": "",
    "text": "This course is targeted at graduate students who plan to work with high-throughput, short-read DNA sequence data. I therefore assume a solid background in genetics as relates to your chosen field. I further assume some proficiency in using the computer, and prior exposure (even if limited) to the basics of scientific programming in R or Python. To help me adjust my instruction to your needs and current skillset, I would like you to answer the following questions in a separate document and email me the answers. (This will not be graded, but will count towards your participation grade.)\n\n\n\nWhat is the Central Dogma?\nWhat is DNA? What are the four nucleotides?\nWhat is RNA? What are the four ribonucleotides?\nWhere do you find DNA in an organisms?\nWhat is an allele, and what is an allele frequency? How would you calculate an allele frequency with DNA sequence data?\nWhat is a genetic marker?\nWhere does genetic variation come from?\nWhat forces impact patterns of genetic variation?\nWhat is a genome?\nHow big is a mammal genome (to an order of magnitude)?\nWhat is ploidy? Are humans diploid, haploid, or tetraploid?\nWhat is recombination?\nWhat is linkage disequilibrium?\nWhat are three types of mutations?\nWhat is population genetic stucture / population subdivision?\n\n\n\n\n\nWhat kind of computer do you have, and what operating system does it run?\nWhat is memory? How much memory does your computer have?\nWhat is storage? How much storage space does your computer have?\nCreate a folder (or directory) for this course and provide its path below.\nPick a recent research project, class assignment, or work task that required you to create a directory. How did you structure its contents?\nHave you ever used the command line?\nWhat is a computing cluster, and have you ever used one?\nWhat programming languages have you used, if any?\nWhat is a package (or library)?\nHave you ever written a function?\nWhat is version control?\nHave you used Git or GitHub?\nWhat does it mean for an analysis to be reproducible?\nHave you ever archived data?\nWhat is open source software?\n\n\n\n\n\nWhat are you hoping to get out of this class?\nDo you have your own dataset, or will you be working with an example?\nWhat kind of questions do you want to address with data?\nWhat are you worried about in this class, if anything?\nIs there anything else I should know to help you have a good semester?"
  },
  {
    "objectID": "assessment.html#genetics",
    "href": "assessment.html#genetics",
    "title": "BIOE591: Preliminary Assessment",
    "section": "",
    "text": "What is the Central Dogma?\nWhat is DNA? What are the four nucleotides?\nWhat is RNA? What are the four ribonucleotides?\nWhere do you find DNA in an organisms?\nWhat is an allele, and what is an allele frequency? How would you calculate an allele frequency with DNA sequence data?\nWhat is a genetic marker?\nWhere does genetic variation come from?\nWhat forces impact patterns of genetic variation?\nWhat is a genome?\nHow big is a mammal genome (to an order of magnitude)?\nWhat is ploidy? Are humans diploid, haploid, or tetraploid?\nWhat is recombination?\nWhat is linkage disequilibrium?\nWhat are three types of mutations?\nWhat is population genetic stucture / population subdivision?"
  },
  {
    "objectID": "assessment.html#computer-skills",
    "href": "assessment.html#computer-skills",
    "title": "BIOE591: Preliminary Assessment",
    "section": "",
    "text": "What kind of computer do you have, and what operating system does it run?\nWhat is memory? How much memory does your computer have?\nWhat is storage? How much storage space does your computer have?\nCreate a folder (or directory) for this course and provide its path below.\nPick a recent research project, class assignment, or work task that required you to create a directory. How did you structure its contents?\nHave you ever used the command line?\nWhat is a computing cluster, and have you ever used one?\nWhat programming languages have you used, if any?\nWhat is a package (or library)?\nHave you ever written a function?\nWhat is version control?\nHave you used Git or GitHub?\nWhat does it mean for an analysis to be reproducible?\nHave you ever archived data?\nWhat is open source software?"
  },
  {
    "objectID": "assessment.html#open",
    "href": "assessment.html#open",
    "title": "BIOE591: Preliminary Assessment",
    "section": "",
    "text": "What are you hoping to get out of this class?\nDo you have your own dataset, or will you be working with an example?\nWhat kind of questions do you want to address with data?\nWhat are you worried about in this class, if anything?\nIs there anything else I should know to help you have a good semester?"
  }
]